{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2tzuflo170yh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tzuflo170yh",
    "outputId": "bb1ae048-6989-4c5e-e526-999b4d9259e4"
   },
   "outputs": [],
   "source": [
    "    # Colab Enviornment\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# sample_df = pd.read_csv(\"/content/drive/MyDrive/ml_class/kaggle_1/submission_sample.csv\")\n",
    "# train_df = pd.read_csv(\"/content/drive/MyDrive/ml_class/kaggle_1/train_dataset.csv\")\n",
    "# test_sample_df = pd.read_csv(\"/content/drive/MyDrive/ml_class/kaggle_1/test_sample.csv\")\n",
    "# test_df = pd.read_csv('/content/drive/MyDrive/ml_class/kaggle_1/test_dataset.csv')\n",
    "# station_info_df = pd.read_csv(\"/content/drive/MyDrive/ml_class/kaggle_1/station_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "327c06b4-318c-4ed5-ad93-025021c39b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "U5WaQ71sJQti",
   "metadata": {
    "id": "U5WaQ71sJQti"
   },
   "outputs": [],
   "source": [
    "# Local Enviornment\n",
    "\n",
    "sample_df = pd.read_csv(\"submission_sample.csv\")\n",
    "train_df = pd.read_csv(\"train_dataset.csv\")\n",
    "test_df = pd.read_csv('test_dataset.csv')\n",
    "station_info_df = pd.read_csv(\"station_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5242f85b-17f1-4a33-a4b5-782e2a000dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns\n",
    "\n",
    "# n24 columns\n",
    "    # cloud_cover_n: 증하층운량(10분위)\n",
    "    # dew_point_n: 이슬점 온도(°C)\n",
    "    # humidity_n: 습도(%)\n",
    "    # local_pressure_n: 현지기압(hPa)\n",
    "    # min_cloud_height_n: 최저운고(100m)\n",
    "    # precipitation_n: 강수량(mm)\n",
    "    # sea_level_pressure_n: 해면기압(hPa)\n",
    "    # snow_depth_n: 적설(cm)\n",
    "    # sunshine_duration_n: 일조(hr)\n",
    "    # surface_temp_n: 지면온도(°C)\n",
    "    # vapor_pressure_n: 증기압(hPa)\n",
    "    # visibility_n: 시정(10m)\n",
    "    # wind_speed_n: 풍속(m/s)\n",
    "\n",
    "# Special comumns\n",
    "    # wind_direction_n\n",
    "    # climatology_temp\n",
    "    # date\n",
    "    # station\n",
    "    #     station number\n",
    "    #     Convert to latitude, longitude(maybe drop), elevation, \n",
    "\n",
    "# Others\n",
    "    # id\n",
    "    # station_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2ff6da4-f7d9-42e0-b869-6c7febbe6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, date_col='date'):\n",
    "        self.date_col = date_col\n",
    "        self.days_before_month = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]  # for non-leap year\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Convert to datetime\n",
    "        df['date_dt'] = pd.to_datetime('2000-' + df[self.date_col], format='%Y-%m-%d')\n",
    "    \n",
    "        # Extract raw date parts\n",
    "        df['month'] = df['date_dt'].dt.month\n",
    "        df['day'] = df['date_dt'].dt.day\n",
    "        df['day_of_year'] = df.apply(lambda row: self.days_before_month[row['month'] - 1] + row['day'], axis=1)\n",
    "        df['day_of_week'] = df['date_dt'].dt.weekday\n",
    "        df['week_of_year'] = df['date_dt'].dt.isocalendar().week.astype('int32')\n",
    "        df['quarter'] = df['date_dt'].dt.quarter\n",
    "    \n",
    "        # Cyclical encodings\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "        df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    \n",
    "        # df['doy_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "        # df['doy_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "    \n",
    "        df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "        df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "        df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "    \n",
    "        df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
    "        df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
    "\n",
    "        # Extract raw date parts\n",
    "        df['month'] = df['date_dt'].dt.month.astype('category')\n",
    "        df['day'] = df['date_dt'].dt.day.astype('category')\n",
    "        df['day_of_week'] = df['date_dt'].dt.weekday.astype('category')\n",
    "        df['week_of_year'] = df['date_dt'].dt.isocalendar().week.astype('int32')\n",
    "        df['quarter'] = df['date_dt'].dt.quarter.astype('category')\n",
    "    \n",
    "        # Drop raw versions\n",
    "        df.drop(columns=[self.date_col, 'date_dt'], inplace=True)\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82322ae9-50fa-4869-9a5f-2fad7c25118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_cols):\n",
    "        self.drop_cols = drop_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Nothing to learn\n",
    "\n",
    "    def transform(self, X):\n",
    "        # vapor_pressure_n = [f\"vapor_pressure_{i}\" for i in range(24)]\n",
    "\n",
    "        final_drop_cols = self.drop_cols\n",
    "\n",
    "        return X.drop(columns=final_drop_cols, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fc83ca4-723e-400b-b7a7-042b04dad7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_col_map = {\n",
    "    \"cloud_cover_n\":        [f\"cloud_cover_{i}\" for i in range(24)],\n",
    "    \"dew_point_n\":          [f\"dew_point_{i}\" for i in range(24)],\n",
    "    \"humidity_n\":           [f\"humidity_{i}\" for i in range(24)],\n",
    "    \"local_pressure_n\":     [f\"local_pressure_{i}\" for i in range(24)],\n",
    "    \"min_cloud_height_n\":   [f\"min_cloud_height_{i}\" for i in range(24)],\n",
    "    \"precipitation_n\":      [f\"precipitation_{i}\" for i in range(24)],\n",
    "    \"sea_level_pressure_n\": [f\"sea_level_pressure_{i}\" for i in range(24)],\n",
    "    \"snow_depth_n\":         [f\"snow_depth_{i}\" for i in range(24)],\n",
    "    \"sunshine_duration_n\":  [f\"sunshine_duration_{i}\" for i in range(24)],\n",
    "    \"surface_temp_n\":       [f\"surface_temp_{i}\" for i in range(24)],\n",
    "    \"vapor_pressure_n\":     [f\"vapor_pressure_{i}\" for i in range(24)],\n",
    "    \"visibility_n\":         [f\"visibility_{i}\" for i in range(24)],\n",
    "    \"wind_speed_n\":         [f\"wind_speed_{i}\" for i in range(24)],\n",
    "    \"wind_direction_n\":     [f\"wind_direction_{i}\" for i in range(24)],\n",
    "    \"station_info\":         [\"station\"]\n",
    "}\n",
    "\n",
    "\n",
    "feature_config = {\n",
    "    \"station_info\": {\n",
    "        \"add_station_info\": False\n",
    "    },\n",
    "    \"cloud_cover_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": False, \"direction_category\": False,\n",
    "        \"when_min\": True, \"when_max\": True, \"first_condition_hour\": True, \"last_condition_hour\": True, \"total_mean\": True\n",
    "    },\n",
    "    \"dew_point_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": False, \"direction_category\": False,\n",
    "        \"when_min\": True, \"when_max\": True, \"first_condition_hour\": False, \"last_condition_hour\": False, \"estimate_air_temp\" : True\n",
    "    },\n",
    "    \"humidity_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": True, \"direction_category\": False,\n",
    "        \"when_min\": True, \"when_max\": True, \"first_condition_hour\": False, \"last_condition_hour\": False, \"total_mean\": True\n",
    "    },\n",
    "    \"local_pressure_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": True, \"direction_category\": False,\n",
    "        \"when_min\": True, \"when_max\": True, \"first_condition_hour\": False, \"last_condition_hour\": False, \"total_mean\": True\n",
    "    },\n",
    "    \"min_cloud_height_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": False, \"direction_category\": False,\n",
    "        \"when_min\": False, \"when_max\": False, \"first_condition_hour\": False, \"last_condition_hour\": False, \"total_mean\": True\n",
    "    },\n",
    "    \"precipitation_n\": {\n",
    "        \"early_night_sum\": True, \"late_night_sum\": True, \"morning_sum\": True,\n",
    "        \"afternoon_sum\": True, \"evening_sum\": True, \"late_evening_sum\": True,\n",
    "        \"total_sum\": True, \"range\": True, \"direction_category\": False,\n",
    "        \"when_min\": True, \"when_max\": True, \"first_condition_hour\": True, \"last_condition_hour\": True, \"rain_event\" : True\n",
    "    },\n",
    "    \"sea_level_pressure_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": True, \"direction_category\": False,\n",
    "        \"when_min\": False, \"when_max\": False, \"first_condition_hour\": False, \"last_condition_hour\": False, \"total_mean\": True\n",
    "    },\n",
    "    \"snow_depth_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": False, \"direction_category\": False,\n",
    "        \"when_min\": False, \"when_max\": True, \"first_condition_hour\": True, \"last_condition_hour\": True, \"snow_event\" : True\n",
    "    },\n",
    "    \"sunshine_duration_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": False, \"direction_category\": False,\n",
    "        \"when_min\": False, \"when_max\": False, \"first_condition_hour\": True, \"last_condition_hour\": True, \"total_mean\": True\n",
    "    },\n",
    "    \"surface_temp_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": True, \"direction_category\": False,\n",
    "        \"when_min\": True, \"when_max\": True, \"first_condition_hour\": False, \"last_condition_hour\": False, \"total_mean\": True\n",
    "    },\n",
    "    \"vapor_pressure_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": True, \"direction_category\": False,\n",
    "        \"when_min\": True, \"when_max\": True, \"first_condition_hour\": False, \"last_condition_hour\": False\n",
    "    },\n",
    "    \"visibility_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": True, \"direction_category\": False,\n",
    "        \"when_min\": True, \"when_max\": True, \"first_condition_hour\": False, \"last_condition_hour\": False, \"total_mean\": True\n",
    "    },\n",
    "    \"wind_speed_n\": {\n",
    "        \"early_night_mean\": True, \"late_night_mean\": True, \"morning_mean\": True,\n",
    "        \"afternoon_mean\": True, \"evening_mean\": True, \"late_evening_mean\": True,\n",
    "        \"total_sum\": False, \"range\": True, \"direction_category\": False,\n",
    "        \"when_min\": True, \"when_max\": True, \"first_condition_hour\": False, \"last_condition_hour\": False, \"total_mean\": True\n",
    "    },\n",
    "    \"wind_direction_n\": {\n",
    "        \"early_night_mean\": False, \"late_night_mean\": False, \"morning_mean\": False,\n",
    "        \"afternoon_mean\": False, \"evening_mean\": False, \"late_evening_mean\": False,\n",
    "        \"total_sum\": False, \"range\": False, \"direction_category\": False,\n",
    "        \"when_min\": False, \"when_max\": False, \"first_condition_hour\": False, \"last_condition_hour\": False, \"total_mean\": False\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a58aac2-29aa-4343-854f-37b050292cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Condenser(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_name, hour_cols, config):\n",
    "        self.feature_name = feature_name\n",
    "        self.hour_cols = hour_cols\n",
    "        self.config = config\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No training needed\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X[self.hour_cols].copy()\n",
    "        result = pd.DataFrame(index=X.index)\n",
    "\n",
    "        if self.config.get(\"add_station_info\"):\n",
    "            # Merge station metadata into df based on 'station' ↔ '지점'\n",
    "            merged = df.merge(\n",
    "                station_info_df[[\"지점\", \"위도\", \"경도\", \"노장해발고도(m)\", '기압계(관측장비지상높이(m))']],\n",
    "                how=\"left\",\n",
    "                left_on=\"station\",\n",
    "                right_on=\"지점\"\n",
    "            )\n",
    "        \n",
    "            result[f\"{self.feature_name}_lat\"] = merged[\"위도\"]\n",
    "            result[f\"{self.feature_name}_height\"] = merged[\"노장해발고도(m)\"]\n",
    "\n",
    "            # Early Night (0–3)\n",
    "        if self.config.get(\"early_night_mean\"):\n",
    "            idx = [i for i in range(0, 4)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_early_night_mean\"] = df[cols].mean(axis=1)\n",
    "    \n",
    "        # Late Night (4–7)\n",
    "        if self.config.get(\"late_night_mean\"):\n",
    "            idx = [i for i in range(4, 8)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_late_night_mean\"] = df[cols].mean(axis=1)\n",
    "    \n",
    "        # Morning (8–11)\n",
    "        if self.config.get(\"morning_mean\"):\n",
    "            idx = [i for i in range(8, 12)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_morning_mean\"] = df[cols].mean(axis=1)\n",
    "    \n",
    "        # Afternoon (12–15)\n",
    "        if self.config.get(\"afternoon_mean\"):\n",
    "            idx = [i for i in range(12, 16)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_afternoon_mean\"] = df[cols].mean(axis=1)\n",
    "    \n",
    "        # Evening (16–19)\n",
    "        if self.config.get(\"evening_mean\"):\n",
    "            idx = [i for i in range(16, 20)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_evening_mean\"] = df[cols].mean(axis=1)\n",
    "    \n",
    "        # Late Evening (20–23)\n",
    "        if self.config.get(\"late_evening_mean\"):\n",
    "            idx = [i for i in range(20, 24)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_late_evening_mean\"] = df[cols].mean(axis=1)\n",
    "\n",
    "        # Sum – Early Night (0–3)\n",
    "        if self.config.get(\"early_night_sum\"):\n",
    "            idx = [i for i in range(0, 4)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_early_night_sum\"] = df[cols].sum(axis=1)\n",
    "        \n",
    "        # Sum – Late Night (4–7)\n",
    "        if self.config.get(\"late_night_sum\"):\n",
    "            idx = [i for i in range(4, 8)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_late_night_sum\"] = df[cols].sum(axis=1)\n",
    "        \n",
    "        # Sum – Morning (8–11)\n",
    "        if self.config.get(\"morning_sum\"):\n",
    "            idx = [i for i in range(8, 12)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_morning_sum\"] = df[cols].sum(axis=1)\n",
    "        \n",
    "        # Sum – Afternoon (12–15)\n",
    "        if self.config.get(\"afternoon_sum\"):\n",
    "            idx = [i for i in range(12, 16)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_afternoon_sum\"] = df[cols].sum(axis=1)\n",
    "        \n",
    "        # Sum – Evening (16–19)\n",
    "        if self.config.get(\"evening_sum\"):\n",
    "            idx = [i for i in range(16, 20)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_evening_sum\"] = df[cols].sum(axis=1)\n",
    "        \n",
    "        # Sum – Late Evening (20–23)\n",
    "        if self.config.get(\"late_evening_sum\"):\n",
    "            idx = [i for i in range(20, 24)]\n",
    "            cols = [self.hour_cols[i] for i in idx]\n",
    "            result[f\"{self.feature_name}_late_evening_sum\"] = df[cols].sum(axis=1)\n",
    "\n",
    "        # First and Last Condition Hour\n",
    "        if self.config.get(\"first_condition_hour\") or self.config.get(\"last_condition_hour\"):\n",
    "            cols = self.hour_cols  # list of 24 hourly columns\n",
    "            valid_mask = (df[cols].notna()) & (df[cols] != 0)  # treat 0 as invalid too\n",
    "            valid_counts = valid_mask.sum(axis=1)  # how many valid values per row?\n",
    "        \n",
    "            if self.config.get(\"first_condition_hour\"):\n",
    "                first_hour = valid_mask.idxmax(axis=1)\n",
    "                first_hour[valid_counts == 0] = np.nan  # force NaN when no valid data\n",
    "                result[f\"{self.feature_name}_first_condition_hour\"] = (\n",
    "                    first_hour.str.extract(r\"_(\\d+)$\").astype(float)\n",
    "                )\n",
    "        \n",
    "            if self.config.get(\"last_condition_hour\"):\n",
    "                last_hour = valid_mask.iloc[:, ::-1].idxmax(axis=1)\n",
    "                last_hour[valid_counts == 0] = np.nan  # force NaN when no valid data\n",
    "                result[f\"{self.feature_name}_last_condition_hour\"] = (\n",
    "                    last_hour.str.extract(r\"_(\\d+)$\").astype(float)\n",
    "                )\n",
    "\n",
    "       # When max\n",
    "        if self.config.get(\"when_max\"):\n",
    "            cols = self.hour_cols\n",
    "            valid_mask = df[cols].notna().sum(axis=1) > 0\n",
    "            max_idx = pd.Series(index=df.index, dtype=\"object\")\n",
    "            max_idx[valid_mask] = df.loc[valid_mask, cols].idxmax(axis=1, skipna=True)\n",
    "            result[f\"{self.feature_name}_when_max\"] = (\n",
    "                max_idx.str.extract(r\"_(\\d+)$\")[0].astype(float).where(valid_mask, -1)\n",
    "            )\n",
    "        \n",
    "        # When min\n",
    "        if self.config.get(\"when_min\"):\n",
    "            cols = self.hour_cols\n",
    "            valid_mask = df[cols].notna().sum(axis=1) > 0\n",
    "            min_idx = pd.Series(index=df.index, dtype=\"object\")\n",
    "            min_idx[valid_mask] = df.loc[valid_mask, cols].idxmin(axis=1, skipna=True)\n",
    "            result[f\"{self.feature_name}_when_min\"] = (\n",
    "                min_idx.str.extract(r\"_(\\d+)$\")[0].astype(float).where(valid_mask, -1)\n",
    "            )\n",
    "\n",
    "        \n",
    "        \n",
    "        if self.config.get(\"mean_minus_climatology\"):\n",
    "            result[f\"{self.feature_name}_mean_minus_climatology\"] = df.mean(axis=1) - X['climatology_temp']\n",
    "            \n",
    "        if self.config.get(\"total_mean\"):\n",
    "            result[f\"{self.feature_name}_total_mean\"] = df.mean(axis=1)\n",
    "            \n",
    "        if self.config.get(\"total_sum\"):\n",
    "            result[f\"{self.feature_name}_total_sum\"] = df.sum(axis=1)\n",
    "\n",
    "        if self.config.get(\"range\"):\n",
    "            cols = self.hour_cols\n",
    "            result[f\"{self.feature_name}_range\"] = df[cols].max(axis=1) - df[cols].min(axis=1)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea629843-0a0b-4fdf-87c1-6b1f971efa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCondensationPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, hour_col_map, feature_config):\n",
    "        self.hour_col_map = hour_col_map\n",
    "        self.feature_config = feature_config\n",
    "        self.condensers = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Instantiate and store a condenser for each feature\n",
    "        self.condensers = [\n",
    "            Condenser(feature, hour_cols, self.feature_config[feature])\n",
    "            for feature, hour_cols in self.hour_col_map.items()\n",
    "        ]\n",
    "        # Fit all condensers (usually a no-op unless you add logic inside them)\n",
    "        for condenser in self.condensers:\n",
    "            condenser.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform all condensers and concatenate results\n",
    "        transformed_parts = [\n",
    "            condenser.transform(X) for condenser in self.condensers\n",
    "        ]\n",
    "\n",
    "        # Drop original n24 columns\n",
    "        drop_cols = [col for cols in self.hour_col_map.values() for col in cols]\n",
    "\n",
    "        # Drop them from X\n",
    "        X_cleaned = X.drop(columns=drop_cols)\n",
    "\n",
    "        # Concatenate all transformed parts and cleaned X\n",
    "        return pd.concat(transformed_parts + [X_cleaned], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5649a1c7-7668-4961-801a-a660117937e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQRBasedOutlierCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, base_features):\n",
    "        self.base_features = base_features  # e.g. [\"humidity_n\", \"local_pressure_n\", ...]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "\n",
    "        df.replace(-9999.0, np.nan, inplace=True)\n",
    "        df.replace(20000.0, np.nan, inplace=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f742877f-adc1-4702-85a9-e48fb2777864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ManualFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "         # Fog factor (visibility × humidity)\n",
    "        X[\"fog_factor\"] = X[\"humidity_n_total_mean\"]*X[\"visibility_n_total_mean\"]\n",
    "\n",
    "        # Apparent Temperature\n",
    "        X[\"heat_index_like\"] = X[\"surface_temp_n_total_mean\"] + 0.1*X[\"humidity_n_total_mean\"]\n",
    "\n",
    "        # Wind chill factor\n",
    "        X[\"wind_chill\"] = X[\"surface_temp_n_total_mean\"] - 0.7*X[\"wind_speed_n_total_mean\"]\n",
    "\n",
    "        X.drop(columns=[\"humidity_n_total_mean\", \"visibility_n_total_mean\"], inplace=True)\n",
    "\n",
    "        # Calculate ln(local and sea level pressure)\n",
    "        P_local = X['local_pressure_n_total_mean']\n",
    "        P_sea = X['sea_level_pressure_n_total_mean']\n",
    "        # Avoid division by zero or invalid values\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            X['ln_local_sea_level_pressure'] = -1 / np.log(P_local / P_sea)\n",
    "        X.drop(columns = ['local_pressure_n_total_mean', 'sea_level_pressure_n_total_mean'], inplace = True)\n",
    "        \n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbca90ed-bf21-4e68-ac30-f00b36137071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"drop_features\", DropFeatureTransformer([\"target\", \"id\", \"station_name\"])),\n",
    "    (\"iqr_cleaning\", IQRBasedOutlierCleaner(\n",
    "        base_features=[\n",
    "            \"humidity\", \"local_pressure\", \"precipitation\",\n",
    "            \"sea_level_pressure\", \"surface_temp\",\n",
    "            \"visibility\", \"wind_speed\"\n",
    "        ]\n",
    "    )),\n",
    "    (\"date_features\", DateFeatureExtractor()),\n",
    "\n",
    "    (\"feature_engineering\", FeatureCondensationPipeline(hour_col_map, feature_config)),\n",
    "\n",
    "    (\"manual_features\", ManualFeatureEngineer()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3eadb080-4f83-41fa-9aac-deec82b967b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Function\n",
    "\n",
    "X_train_ori = pipeline.fit_transform(train_df)\n",
    "y_train_ori = train_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_ori, y_train_ori, test_size = 0.2, random_state=42)\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"CatBoost\": CatBoostRegressor(\n",
    "    boosting_type= 'Plain', \n",
    "    grow_policy= 'SymmetricTree', \n",
    "    iterations= 3600, \n",
    "    learning_rate= 0.054179676266114606, \n",
    "    depth= 9, \n",
    "    l2_leaf_reg= 5.425455174508331, \n",
    "    random_strength= 0.632030143949728, \n",
    "    bagging_temperature= 0.9513670435640664, \n",
    "    border_count= 215,\n",
    "        \n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "),\n",
    "\n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        random_state=42,\n",
    "        tree_method='hist',\n",
    "        enable_categorical=True,\n",
    "\n",
    "\n",
    "        n_estimators= 4000, \n",
    "        max_depth= 5, \n",
    "        learning_rate= 0.055025307082608235, \n",
    "        subsample= 0.6272611512543017, \n",
    "        colsample_bytree= 0.9867043517730124, \n",
    "        reg_alpha= 0.5543753187724325, \n",
    "        reg_lambda= 4.41395878314679\n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": LGBMRegressor(\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "\n",
    "        n_estimators= 3000, \n",
    "        max_depth= 11, \n",
    "        learning_rate= 0.09216865202064772, \n",
    "        num_leaves= 28, \n",
    "        min_child_samples= 40, \n",
    "        subsample= 0.5276614407170144, \n",
    "        colsample_bytree= 0.8239904218170874, \n",
    "        reg_alpha= 0.0787097082876258, \n",
    "        reg_lambda=9.831250240403401\n",
    "\n",
    "    ),\n",
    "\n",
    "    \"RandomForest\": RandomForestRegressor(\n",
    "        # n_estimators=100,\n",
    "        n_estimators=900,\n",
    "        max_depth=None,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=0,\n",
    "        max_features = None\n",
    "    ),\n",
    "\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000, random_state=42)\n",
    "\n",
    "}\n",
    "\n",
    "def preprocess_for_model(model_name, X_train, X_test):\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    if model_name == \"CatBoost\":\n",
    "        X_train[\"day\"] = X_train[\"day\"].astype(int)\n",
    "        X_test[\"day\"] = X_test[\"day\"].astype(int)\n",
    "        X_train[\"month\"] = X_train[\"month\"].astype(int)\n",
    "        X_test[\"month\"] = X_test[\"month\"].astype(int)\n",
    "        \n",
    "        # vapor_pressure_n_late_evening_mean times week_cos\n",
    "        X_train[\"vapor_pressure_late_evening_mean_times_week_cos\"] = X_train['vapor_pressure_n_late_evening_mean'] * X_train['week_cos']\n",
    "        X_test[\"vapor_pressure_late_evening_mean_times_week_cos\"] = X_test['vapor_pressure_n_late_evening_mean'] * X_test['week_cos']\n",
    "\n",
    "        # dew_point_n_late_evening_mean humidity_n_late_evening_mean: 2.0209\n",
    "        X_train[\"dew_point_n_late_evening_mean_times_humidity_n_late_evening_mean\"] = X_train['dew_point_n_late_evening_mean'] * X_train['humidity_n_late_evening_mean']\n",
    "        X_test[\"dew_point_n_late_evening_mean_times_humidity_n_late_evening_mean\"] = X_test['dew_point_n_late_evening_mean'] * X_test['humidity_n_late_evening_mean']\n",
    "\n",
    "        # dew_point_n_late_evening_mean * humidity_n_afternoon_mean: 1.5551\n",
    "        X_train[\"dew_point_n_late_evening_mean_times_humidity_n_afternoon_mean\"] = (\n",
    "            X_train[\"dew_point_n_late_evening_mean\"] * X_train[\"humidity_n_afternoon_mean\"]\n",
    "        )\n",
    "        X_test[\"dew_point_n_late_evening_mean_times_humidity_n_afternoon_mean\"] = (\n",
    "            X_test[\"dew_point_n_late_evening_mean\"] * X_test[\"humidity_n_afternoon_mean\"]\n",
    "        )\n",
    "\n",
    "        # Drop features\n",
    "        X_train = X_train.drop(columns=['ln_local_sea_level_pressure'], errors='ignore')\n",
    "        X_test = X_test.drop(columns=['ln_local_sea_level_pressure'], errors='ignore')\n",
    "        \n",
    "\n",
    "        cat_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "        for col in cat_features:\n",
    "            X_train[col] = X_train[col].astype(str).fillna(\"Missing\")\n",
    "            X_test[col] = X_test[col].astype(str).fillna(\"Missing\")\n",
    "\n",
    "    elif model_name == \"LightGBM\":\n",
    "\n",
    "        # humidity_n_late_evening_mean climatology_temp: 31.0000\n",
    "        X_train[\"humidity_n_late_evening_mean_times_climatology_temp\"] = X_train[\"humidity_n_late_evening_mean\"] * X_train[\"climatology_temp\"]\n",
    "        X_test[\"humidity_n_late_evening_mean_times_climatology_temp\"] = X_test[\"humidity_n_late_evening_mean\"] * X_test[\"climatology_temp\"]\n",
    "        \n",
    "        # vapor_pressure_n_late_evening_mean week_cos: 26.0000\n",
    "        X_train[\"vapor_pressure_n_late_evening_mean_times_week_cos\"] = X_train[\"vapor_pressure_n_late_evening_mean\"] * X_train[\"week_cos\"]\n",
    "        X_test[\"vapor_pressure_n_late_evening_mean_times_week_cos\"] = X_test[\"vapor_pressure_n_late_evening_mean\"] * X_test[\"week_cos\"]\n",
    "        \n",
    "        # dew_point_n_late_evening_mean humidity_n_late_evening_mean: 17.0000\n",
    "        X_train[\"dew_point_n_late_evening_mean_times_humidity_n_late_evening_mean\"] = X_train[\"dew_point_n_late_evening_mean\"] * X_train[\"humidity_n_late_evening_mean\"]\n",
    "        X_test[\"dew_point_n_late_evening_mean_times_humidity_n_late_evening_mean\"] = X_test[\"dew_point_n_late_evening_mean\"] * X_test[\"humidity_n_late_evening_mean\"]\n",
    "        \n",
    "        # dew_point_n_late_evening_mean month_cos: 15.0000\n",
    "        X_train[\"dew_point_n_late_evening_mean_times_month_cos\"] = X_train[\"dew_point_n_late_evening_mean\"] * X_train[\"month_cos\"]\n",
    "        X_test[\"dew_point_n_late_evening_mean_times_month_cos\"] = X_test[\"dew_point_n_late_evening_mean\"] * X_test[\"month_cos\"]\n",
    "        \n",
    "        # vapor_pressure_n_late_evening_mean week_sin: 14.0000\n",
    "        X_train[\"vapor_pressure_n_late_evening_mean_times_week_sin\"] = X_train[\"vapor_pressure_n_late_evening_mean\"] * X_train[\"week_sin\"]\n",
    "        X_test[\"vapor_pressure_n_late_evening_mean_times_week_sin\"] = X_test[\"vapor_pressure_n_late_evening_mean\"] * X_test[\"week_sin\"]\n",
    "        \n",
    "        # climatology_temp week_of_year: 14.0000\n",
    "        X_train[\"climatology_temp_times_week_of_year\"] = X_train[\"climatology_temp\"] * X_train[\"week_of_year\"]\n",
    "        X_test[\"climatology_temp_times_week_of_year\"] = X_test[\"climatology_temp\"] * X_test[\"week_of_year\"]\n",
    "        \n",
    "        # surface_temp_n_late_evening_mean week_sin: 13.0000\n",
    "        X_train[\"surface_temp_n_late_evening_mean_times_week_sin\"] = X_train[\"surface_temp_n_late_evening_mean\"] * X_train[\"week_sin\"]\n",
    "        X_test[\"surface_temp_n_late_evening_mean_times_week_sin\"] = X_test[\"surface_temp_n_late_evening_mean\"] * X_test[\"week_sin\"]\n",
    "        \n",
    "        # climatology_temp week_cos: 13.0000\n",
    "        X_train[\"climatology_temp_times_week_cos\"] = X_train[\"climatology_temp\"] * X_train[\"week_cos\"]\n",
    "        X_test[\"climatology_temp_times_week_cos\"] = X_test[\"climatology_temp\"] * X_test[\"week_cos\"]\n",
    "        \n",
    "        # humidity_n_evening_mean climatology_temp: 12.0000\n",
    "        X_train[\"humidity_n_evening_mean_times_climatology_temp\"] = X_train[\"humidity_n_evening_mean\"] * X_train[\"climatology_temp\"]\n",
    "        X_test[\"humidity_n_evening_mean_times_climatology_temp\"] = X_test[\"humidity_n_evening_mean\"] * X_test[\"climatology_temp\"]\n",
    "        \n",
    "        # dew_point_n_late_evening_mean week_cos: 11.0000\n",
    "        X_train[\"dew_point_n_late_evening_mean_times_week_cos\"] = X_train[\"dew_point_n_late_evening_mean\"] * X_train[\"week_cos\"]\n",
    "        X_test[\"dew_point_n_late_evening_mean_times_week_cos\"] = X_test[\"dew_point_n_late_evening_mean\"] * X_test[\"week_cos\"]\n",
    "\n",
    "        # 🔥 Drop range features\n",
    "        range_cols = [col for col in X_train.columns if col.endswith(\"_range\")]\n",
    "        X_train = X_train.drop(columns=range_cols)\n",
    "        X_test = X_test.drop(columns=range_cols)\n",
    "\n",
    "        slope_cols = [col for col in X_train.columns if col.endswith(\"_hourly_linear_slope\")]\n",
    "        X_train = X_train.drop(columns=slope_cols)\n",
    "        X_test = X_test.drop(columns=slope_cols)\n",
    "\n",
    "        when_cols = [col for col in X_train.columns if col.endswith(\"_when_max\") or col.endswith(\"_when_min\")]\n",
    "        X_train = X_train.drop(columns=when_cols, errors='ignore')\n",
    "        X_test = X_test.drop(columns=when_cols, errors='ignore')\n",
    "\n",
    "        cyclical_cols = [\n",
    "            \"month_sin\", \"month_cos\", \"day_sin\", \"day_cos\", \"doy_sin\", \"doy_cos\", \"dow_sin\", \"dow_cos\", \"week_sin\", \"week_cos\", \"quarter_sin\", \"quarter_cos\"]\n",
    "        \n",
    "        X_train = X_train.drop(columns=cyclical_cols, errors='ignore')\n",
    "        X_test = X_test.drop(columns=cyclical_cols, errors='ignore')\n",
    "\n",
    "        cat_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    elif model_name == \"XGBoost\":\n",
    "\n",
    "        # dew_point_n_late_evening_mean * humidity_n_afternoon_mean: 0.0821\n",
    "        X_train[\"dew_point_n_late_evening_mean_times_humidity_n_afternoon_mean\"] = (\n",
    "            X_train[\"dew_point_n_late_evening_mean\"] * X_train[\"humidity_n_afternoon_mean\"]\n",
    "        )\n",
    "        X_test[\"dew_point_n_late_evening_mean_times_humidity_n_afternoon_mean\"] = (\n",
    "            X_test[\"dew_point_n_late_evening_mean\"] * X_test[\"humidity_n_afternoon_mean\"]\n",
    "        )\n",
    "    \n",
    "        # wind_speed_n_range * week_of_year: 0.0219\n",
    "        X_train[\"wind_speed_n_range_times_week_of_year\"] = (\n",
    "            X_train[\"wind_speed_n_range\"] * X_train[\"week_of_year\"]\n",
    "        )\n",
    "        X_test[\"wind_speed_n_range_times_week_of_year\"] = (\n",
    "            X_test[\"wind_speed_n_range\"] * X_test[\"week_of_year\"]\n",
    "        )\n",
    "    \n",
    "        # vapor_pressure_n_late_evening_mean * week_cos: 0.0203\n",
    "        X_train[\"vapor_pressure_n_late_evening_mean_times_week_cos\"] = (\n",
    "            X_train[\"vapor_pressure_n_late_evening_mean\"] * X_train[\"week_cos\"]\n",
    "        )\n",
    "        X_test[\"vapor_pressure_n_late_evening_mean_times_week_cos\"] = (\n",
    "            X_test[\"vapor_pressure_n_late_evening_mean\"] * X_test[\"week_cos\"]\n",
    "        )\n",
    "    \n",
    "        # snow_depth_n_when_max * week_sin: 0.0185\n",
    "        X_train[\"snow_depth_n_when_max_times_week_sin\"] = (\n",
    "            X_train[\"snow_depth_n_when_max\"] * X_train[\"week_sin\"]\n",
    "        )\n",
    "        X_test[\"snow_depth_n_when_max_times_week_sin\"] = (\n",
    "            X_test[\"snow_depth_n_when_max\"] * X_test[\"week_sin\"]\n",
    "        )\n",
    "    \n",
    "        # surface_temp_n_evening_mean * month_sin: 0.0149\n",
    "        X_train[\"surface_temp_n_evening_mean_times_month_sin\"] = (\n",
    "            X_train[\"surface_temp_n_evening_mean\"] * X_train[\"month_sin\"]\n",
    "        )\n",
    "        X_test[\"surface_temp_n_evening_mean_times_month_sin\"] = (\n",
    "            X_test[\"surface_temp_n_evening_mean\"] * X_test[\"month_sin\"]\n",
    "        )\n",
    "    \n",
    "        # surface_temp_n_afternoon_mean * quarter_sin: 0.0132\n",
    "        X_train[\"surface_temp_n_afternoon_mean_times_quarter_sin\"] = (\n",
    "            X_train[\"surface_temp_n_afternoon_mean\"] * X_train[\"quarter_sin\"]\n",
    "        )\n",
    "        X_test[\"surface_temp_n_afternoon_mean_times_quarter_sin\"] = (\n",
    "            X_test[\"surface_temp_n_afternoon_mean\"] * X_test[\"quarter_sin\"]\n",
    "        )\n",
    "    \n",
    "        # surface_temp_n_late_evening_mean * quarter: 0.0121\n",
    "        X_train[\"quarter\"] = X_train[\"quarter\"].astype(int)\n",
    "        X_test[\"quarter\"] = X_test[\"quarter\"].astype(int)\n",
    "        X_train[\"surface_temp_n_late_evening_mean_times_quarter\"] = (\n",
    "            X_train[\"surface_temp_n_late_evening_mean\"] * X_train[\"quarter\"]\n",
    "        )\n",
    "        X_test[\"surface_temp_n_late_evening_mean_times_quarter\"] = (\n",
    "            X_test[\"surface_temp_n_late_evening_mean\"] * X_test[\"quarter\"]\n",
    "        )\n",
    "    \n",
    "        # dew_point_n_late_evening_mean * quarter: 0.0119\n",
    "        X_train[\"dew_point_n_late_evening_mean_times_quarter\"] = (\n",
    "            X_train[\"dew_point_n_late_evening_mean\"] * X_train[\"quarter\"]\n",
    "        )\n",
    "        X_test[\"dew_point_n_late_evening_mean_times_quarter\"] = (\n",
    "            X_test[\"dew_point_n_late_evening_mean\"] * X_test[\"quarter\"]\n",
    "        )\n",
    "    \n",
    "        # climatology_temp * fog_factor: 0.0111\n",
    "        X_train[\"climatology_temp_times_fog_factor\"] = (\n",
    "            X_train[\"climatology_temp\"] * X_train[\"fog_factor\"]\n",
    "        )\n",
    "        X_test[\"climatology_temp_times_fog_factor\"] = (\n",
    "            X_test[\"climatology_temp\"] * X_test[\"fog_factor\"]\n",
    "        )\n",
    "    \n",
    "        # dew_point_n_late_evening_mean * humidity_n_late_evening_mean: 0.0108\n",
    "        X_train[\"dew_point_n_late_evening_mean_times_humidity_n_late_evening_mean\"] = (\n",
    "            X_train[\"dew_point_n_late_evening_mean\"] * X_train[\"humidity_n_late_evening_mean\"]\n",
    "        )\n",
    "        X_test[\"dew_point_n_late_evening_mean_times_humidity_n_late_evening_mean\"] = (\n",
    "            X_test[\"dew_point_n_late_evening_mean\"] * X_test[\"humidity_n_late_evening_mean\"]\n",
    "        )\n",
    "\n",
    "        # # 🔥 Drop range features + ln_local_sea_level_pressure\n",
    "        range_cols = [col for col in X_train.columns if col.endswith(\"_range\")]\n",
    "        range_cols = range_cols + ['ln_local_sea_level_pressure']\n",
    "        X_train = X_train.drop(columns=range_cols)\n",
    "        X_test = X_test.drop(columns=range_cols)\n",
    "\n",
    "        when_cols = [col for col in X_train.columns if col.endswith(\"_when_max\") or col.endswith(\"_when_min\")]\n",
    "        X_train = X_train.drop(columns=when_cols)\n",
    "        X_test = X_test.drop(columns=when_cols)\n",
    "\n",
    "        cat_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "\n",
    "    elif model_name == \"RandomForest\":\n",
    "\n",
    "        # Identify all object/category columns\n",
    "        cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "        when_cols = [col for col in X_train.columns if col.endswith(\"_when_max\") or col.endswith(\"_when_min\")]\n",
    "        when_cols = when_cols + ['ln_local_sea_level_pressure']\n",
    "        X_train = X_train.drop(columns=when_cols)\n",
    "        X_test = X_test.drop(columns=when_cols)\n",
    "\n",
    "        # Fill NaNs in categorical columns\n",
    "        for col in cat_cols:\n",
    "            X_train[col] = X_train[col].astype(str).fillna(\"Missing\")\n",
    "            X_test[col] = X_test[col].astype(str).fillna(\"Missing\")\n",
    "    \n",
    "        # One-hot encode ALL categorical columns\n",
    "        X_train = pd.get_dummies(X_train, columns=cat_cols)\n",
    "        X_test = pd.get_dummies(X_test, columns=cat_cols)\n",
    "    \n",
    "        # Align columns across train/test to avoid mismatch\n",
    "        X_train, X_test = X_train.align(X_test, join=\"outer\", axis=1, fill_value=0)\n",
    "    \n",
    "        # Fill NaNs in numeric columns with -1\n",
    "        num_cols = X_train.select_dtypes(include=[\"float\", \"int\"]).columns\n",
    "        X_train[num_cols] = X_train[num_cols].fillna(-1)\n",
    "        X_test[num_cols] = X_test[num_cols].fillna(-1)\n",
    "    \n",
    "        cat_features = []  # not needed for RF\n",
    "\n",
    "    elif model_name in [\"Ridge\", \"ElasticNet\", \"SVR\"]:\n",
    "        # Fill NaNs with column-wise means (numeric only)\n",
    "        X_train = X_train.fillna(X_train.mean(numeric_only=True))\n",
    "        X_test = X_test.fillna(X_train.mean(numeric_only=True))  # use train stats\n",
    "    \n",
    "        # Encode categorical columns using one-hot\n",
    "        cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        X_train = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)\n",
    "        X_test = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)\n",
    "    \n",
    "        # Align train/test columns (important after encoding!)\n",
    "        X_train, X_test = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "        X_train = X_train.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "\n",
    "        cat_features = []\n",
    "\n",
    "\n",
    "    return X_train, X_test, cat_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffafd0d1-ffe1-458c-ae71-bd8f3c08ea28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost R² score: 0.8525\n",
      "XGBoost R² score: 0.8679\n",
      "LightGBM R² score: 0.8603\n",
      "RandomForest R² score: 0.7550\n",
      "Ridge R² score: 0.6586\n",
      "ElasticNet R² score: 0.6390\n",
      "Level 1 meta model RidgeCV R² score: 0.8751\n"
     ]
    }
   ],
   "source": [
    "# Main Prediction\n",
    "\n",
    "# Prediction for Individual Models\n",
    "for name, model in models.items():\n",
    "    cat_features = []\n",
    "    X_train_mod, X_test_mod, cat_features = preprocess_for_model(name, X_train, X_test)\n",
    "\n",
    "    if name == \"CatBoost\":\n",
    "        model.fit(X_train_mod, y_train, cat_features=cat_features)\n",
    "\n",
    "    elif name == \"LightGBM\":\n",
    "        model.fit(X_train_mod, y_train, categorical_feature=cat_features)\n",
    "\n",
    "    elif name in [\"XGBoost\", \"RandomForest\", \"Ridge\", \"ElasticNet\", \"SVR\"]:\n",
    "        model.fit(X_train_mod, y_train)\n",
    "\n",
    "    preds = model.predict(X_test_mod)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name} R² score: {r2:.4f}\")\n",
    "\n",
    "    # # Check for feature importance support\n",
    "    # if hasattr(model, \"feature_importances_\"):\n",
    "    #     importances = np.array(model.feature_importances_)\n",
    "    #     features = X_train_mod.columns\n",
    "\n",
    "    #     # Sort top 30\n",
    "    #     sorted_indices = importances.argsort()[::-1][:20]\n",
    "    #     print(f\"\\nTop 30 features for {name}:\")\n",
    "    #     for i in sorted_indices:\n",
    "    #         print(f\"{features[i]}: {importances[i]:.4f}\")\n",
    "    # else:\n",
    "    #     print(f\"{name} does not support feature_importances_\")\n",
    "\n",
    "    # print()\n",
    "\n",
    "\n",
    "\n",
    "# === Level 1 meta model Prediction===\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "meta_models = {\n",
    "    \"RidgeCV\": RidgeCV(\n",
    "        alphas=[0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        fit_intercept=True\n",
    "    )\n",
    "}\n",
    "\n",
    "# Out-of-fold predictions for train set\n",
    "train_meta = np.zeros((X_train.shape[0], len(models)))\n",
    "test_meta = np.zeros((X_test.shape[0], len(models)))\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    test_meta_fold = np.zeros((X_test.shape[0], n_folds))\n",
    "\n",
    "    # Preprocess X_test ONCE per model\n",
    "    X_test_preprocessed = preprocess_for_model(name, X_train, X_test)[1]\n",
    "\n",
    "\n",
    "    for j, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        X_tr_raw = X_train.iloc[train_idx]\n",
    "        X_val_raw = X_train.iloc[val_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "\n",
    "        # Preprocess fold data\n",
    "        cat_features = []\n",
    "        X_tr, X_val, cat_features = preprocess_for_model(name, X_tr_raw, X_val_raw)\n",
    "        # Align all 3 sets to ensure identical columns\n",
    "        X_tr, X_val = X_tr.align(X_val, join=\"outer\", axis=1, fill_value=0)\n",
    "        X_tr, X_test_preprocessed = X_tr.align(X_test_preprocessed, join=\"outer\", axis=1, fill_value=0)\n",
    "        X_val, X_test_preprocessed = X_val.align(X_test_preprocessed, join=\"outer\", axis=1, fill_value=0)\n",
    "\n",
    "        model_clone = clone(model)\n",
    "\n",
    "        if name == \"LightGBM\":\n",
    "            model_clone.fit(X_tr, y_tr, categorical_feature=cat_features)\n",
    "        elif name == \"CatBoost\":\n",
    "            model_clone.fit(X_tr, y_tr, cat_features=cat_features)\n",
    "        else:\n",
    "            model_clone.fit(X_tr, y_tr)\n",
    "\n",
    "        train_meta[val_idx, i] = model_clone.predict(X_val)\n",
    "        test_meta_fold[:, j] = model_clone.predict(X_test_preprocessed)\n",
    "\n",
    "    # Average test predictions across folds\n",
    "    test_meta[:, i] = test_meta_fold.mean(axis=1)\n",
    "\n",
    "# Fit and evaluate each meta-model\n",
    "for name, model in meta_models.items():\n",
    "    model.fit(train_meta, y_train)\n",
    "    stack_preds = model.predict(test_meta)\n",
    "    stack_r2 = r2_score(y_test, stack_preds)\n",
    "    print(f\"Level 1 meta model {name} R² score: {stack_r2:.4f}\")\n",
    "\n",
    "\n",
    "# Submit logic\n",
    "# Out-of-fold predictions for train set\n",
    "\n",
    "X_train = pipeline.fit_transform(train_df)\n",
    "y_train = train_df['target']\n",
    "X_test = pipeline.fit_transform(test_df)\n",
    "\n",
    "train_meta = np.zeros((X_train.shape[0], len(models)))\n",
    "test_meta = np.zeros((X_test.shape[0], len(models)))\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    test_meta_fold = np.zeros((X_test.shape[0], n_folds))\n",
    "\n",
    "    # Preprocess X_test ONCE per model\n",
    "    X_test_preprocessed = preprocess_for_model(name, X_train, X_test)[1]\n",
    "\n",
    "\n",
    "    for j, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        X_tr_raw = X_train.iloc[train_idx]\n",
    "        X_val_raw = X_train.iloc[val_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "\n",
    "        # Preprocess fold data\n",
    "        cat_features = []\n",
    "        X_tr, X_val, cat_features = preprocess_for_model(name, X_tr_raw, X_val_raw)\n",
    "        # Align all 3 sets to ensure identical columns\n",
    "        X_tr, X_val = X_tr.align(X_val, join=\"outer\", axis=1, fill_value=0)\n",
    "        X_tr, X_test_preprocessed = X_tr.align(X_test_preprocessed, join=\"outer\", axis=1, fill_value=0)\n",
    "        X_val, X_test_preprocessed = X_val.align(X_test_preprocessed, join=\"outer\", axis=1, fill_value=0)\n",
    "\n",
    "        model_clone = clone(model)\n",
    "\n",
    "        if name == \"LightGBM\":\n",
    "            model_clone.fit(X_tr, y_tr, categorical_feature=cat_features)\n",
    "        elif name == \"CatBoost\":\n",
    "            model_clone.fit(X_tr, y_tr, cat_features=cat_features)\n",
    "        else:\n",
    "            model_clone.fit(X_tr, y_tr)\n",
    "\n",
    "        train_meta[val_idx, i] = model_clone.predict(X_val)\n",
    "        test_meta_fold[:, j] = model_clone.predict(X_test_preprocessed)\n",
    "\n",
    "    # Average test predictions across folds\n",
    "    test_meta[:, i] = test_meta_fold.mean(axis=1)\n",
    "\n",
    "# Fit and evaluate each meta-model\n",
    "for name, model in meta_models.items():\n",
    "    model.fit(train_meta, y_train)\n",
    "    stack_preds = model.predict(test_meta)\n",
    "    # stack_r2 = r2_score(y_test, stack_preds)\n",
    "    # print(f\"Level 1 meta model {name} R² score: {stack_r2:.4f}\")\n",
    "    submit_df = pd.DataFrame({'id': range(len(stack_preds)), 'target':stack_preds})\n",
    "    submit_df.to_csv('submit.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b36ebb-2e65-466e-b005-42bf6496fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost R² score: 0.8525\n",
    "XGBoost R² score: 0.8679\n",
    "LightGBM R² score: 0.8603\n",
    "RandomForest R² score: 0.7550\n",
    "Ridge R² score: 0.6586\n",
    "ElasticNet R² score: 0.6390\n",
    "Level 1 meta model RidgeCV R² score: 0.8751\n",
    "\n",
    "# Last hyperparameter tuning\n",
    "CatBoost R² score: 0.8525\n",
    "XGBoost R² score: 0.8679\n",
    "LightGBM R² score: 0.8603\n",
    "RandomForest R² score: 0.7550\n",
    "Level 1 meta model RidgeCV R² score: 0.8739\n",
    "\n",
    "# With station info\n",
    "CatBoost R² score: 0.8525\n",
    "XGBoost R² score: 0.8706\n",
    "LightGBM R² score: 0.8617\n",
    "RandomForest R² score: 0.7308\n",
    "Level 1 meta model RidgeCV R² score: 0.8752\n",
    "\n",
    "# After hyper parameter tuning\n",
    "CatBoost R² score: 0.8536\n",
    "XGBoost R² score: 0.8631\n",
    "LightGBM R² score: 0.8572\n",
    "RandomForest R² score: 0.7544\n",
    "Level 1 meta model RidgeCV R² score: 0.8716\n",
    "\n",
    "CatBoost R² score: 0.8120\n",
    "XGBoost R² score: 0.8121\n",
    "LightGBM R² score: 0.7900\n",
    "RandomForest R² score: 0.7296\n",
    "Level 1 meta model RidgeCV R² score: 0.8343\n",
    "\n",
    "CatBoost R² score: 0.8142\n",
    "XGBoost R² score: 0.8099\n",
    "LightGBM R² score: 0.7877\n",
    "RandomForest R² score: 0.7319\n",
    "Level 1 meta model RidgeCV R² score: 0.8332\n",
    "\n",
    "CatBoost R² score: 0.8102\n",
    "XGBoost R² score: 0.8109\n",
    "LightGBM R² score: 0.7893\n",
    "RandomForest R² score: 0.7318\n",
    "Level 1 meta model RidgeCV R² score: 0.8316\n",
    "\n",
    "CatBoost R² score: 0.8110\n",
    "XGBoost R² score: 0.8114\n",
    "LightGBM R² score: 0.7810\n",
    "RandomForest R² score: 0.7502\n",
    "Level 1 meta model RidgeCV R² score: 0.8308\n",
    "\n",
    "# After hyper parameter tuning\n",
    "CatBoost R² score: 0.8536\n",
    "XGBoost R² score: 0.8631\n",
    "LightGBM R² score: 0.8572\n",
    "RandomForest R² score: 0.7544\n",
    "Level 1 meta model RidgeCV R² score: 0.8716\n",
    "\n",
    "CatBoost R² score: 0.8110\n",
    "XGBoost R² score: 0.8114\n",
    "LightGBM R² score: 0.7715\n",
    "RandomForest R² score: 0.7542\n",
    "Level 1 meta model RidgeCV R² score: 0.8307\n",
    "\n",
    "CatBoost R² score: 0.8100\n",
    "XGBoost R² score: 0.8049\n",
    "LightGBM R² score: 0.7715\n",
    "RandomForest R² score: 0.7542\n",
    "Level 1 meta model RidgeCV R² score: 0.8332\n",
    "\n",
    "CatBoost R² score: 0.8047\n",
    "XGBoost R² score: 0.8049\n",
    "LightGBM R² score: 0.7715\n",
    "RandomForest R² score: 0.7542\n",
    "Level 1 meta model RidgeCV R² score: 0.8329\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6377cf0-9841-4e14-81a6-663c1c26c12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13132, 9179)\n",
      "CatBoost R² score: 0.8552\n",
      "\n",
      "📌 Training CatBoost on full feature set...\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Feature generation and printing important features\n",
    "\n",
    "X_train_ori = pipeline.fit_transform(train_df)\n",
    "y_train_ori = train_df['target']\n",
    "\n",
    "\n",
    "X_poly_process = X_train_ori.copy()\n",
    "cat_cols = X_poly_process.select_dtypes(['category']).columns\n",
    "X_poly_process[cat_cols] = X_poly_process[cat_cols].astype('int')\n",
    "X_poly_process = X_poly_process.fillna(0)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_poly_process)\n",
    "\n",
    "feature_names = poly.get_feature_names_out(X_poly_process.columns)\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=feature_names)\n",
    "print(X_poly_df.shape) # (13132, 9179) \n",
    "# display(X_poly_df.T.head(400))\n",
    "\n",
    "# Step 1: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly_df, y_train_ori, test_size=0.2, random_state=42\n",
    ")\n",
    "# print(X_train.shape)\n",
    "\n",
    "models = {\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42,\n",
    "                                      boosting_type= 'Plain', \n",
    "    grow_policy= 'SymmetricTree', \n",
    "    iterations= 2200, \n",
    "    learning_rate= 0.10479224744093014,\n",
    "    depth= 8, \n",
    "    l2_leaf_reg= 8.969716675882767, \n",
    "    random_strength= 1.05507240655598, \n",
    "    bagging_temperature= 0.3332356005734044, \n",
    "    border_count= 74,\n",
    "        \n",
    "                                 ),\n",
    "    # \"XGBoost\": XGBRegressor(\n",
    "    #     n_estimators=100,\n",
    "    #     random_state=42,\n",
    "    #     tree_method='hist',\n",
    "    #     enable_categorical=True),\n",
    "\n",
    "    # \"LightGBM\": LGBMRegressor(\n",
    "    #     n_estimators=100,\n",
    "    #     random_state=42,\n",
    "    #     verbose=-1)\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    cat_9000_trained = model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name} R² score: {r2:.4f}\")\n",
    "\n",
    "# Store R2 and top 3000 DataFrames\n",
    "r2_scores_full = {}\n",
    "X_train_top3000_dict = {}\n",
    "top3000_features_dict = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n📌 Training {name} on full feature set...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # R2 on full features\n",
    "    preds_full = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, preds_full)\n",
    "    r2_scores_full[name] = r2\n",
    "    print(f\"✅ {name} R² score (full features): {r2:.4f}\")\n",
    "    \n",
    "    # Get top 3000 features\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "        sorted_indices = np.argsort(importances)[::-1][:3000]\n",
    "        top_features = X_train.columns[sorted_indices]\n",
    "        \n",
    "        # Store top 3000 features per model\n",
    "        X_train_top3000_dict[name] = X_train[top_features].copy()\n",
    "        top3000_features_dict[name] = list(top_features)\n",
    "        \n",
    "        print(f\"✅ {name} top 3000 features extracted.\")\n",
    "    else:\n",
    "        print(f\"⚠️ {name} does not support feature_importances_. Skipping top 3000 extraction.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58e93b72-4a63-468a-83c3-e2fb2f6ea204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:44:06,586] A new study created in memory with name: no-name-ba159552-06d1-4dae-9ae9-6bd3009211a6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255634d6775f4808b731a65b049622ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:44:09,842] Trial 0 finished with value: 0.6393483198327363 and parameters: {'n_estimators': 1000, 'max_depth': 41, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': 'log2', 'bootstrap': False}. Best is trial 0 with value: 0.6393483198327363.\n",
      "[I 2025-05-14 17:44:10,689] Trial 1 finished with value: 0.4752017928091845 and parameters: {'n_estimators': 400, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 0 with value: 0.6393483198327363.\n",
      "[I 2025-05-14 17:44:16,478] Trial 2 finished with value: 0.6897486872536919 and parameters: {'n_estimators': 200, 'max_depth': 33, 'min_samples_split': 12, 'min_samples_leaf': 10, 'max_features': None, 'bootstrap': True}. Best is trial 2 with value: 0.6897486872536919.\n",
      "[I 2025-05-14 17:44:16,881] Trial 3 finished with value: 0.30537433127697133 and parameters: {'n_estimators': 300, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 2 with value: 0.6897486872536919.\n",
      "[I 2025-05-14 17:44:18,010] Trial 4 finished with value: 0.5031988100956831 and parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 19, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 2 with value: 0.6897486872536919.\n",
      "[I 2025-05-14 17:44:19,579] Trial 5 finished with value: 0.5012176762300837 and parameters: {'n_estimators': 900, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 9, 'max_features': 'log2', 'bootstrap': True}. Best is trial 2 with value: 0.6897486872536919.\n",
      "[I 2025-05-14 17:44:20,592] Trial 6 finished with value: 0.545779176777778 and parameters: {'n_estimators': 500, 'max_depth': 23, 'min_samples_split': 3, 'min_samples_leaf': 9, 'max_features': 'log2', 'bootstrap': True}. Best is trial 2 with value: 0.6897486872536919.\n",
      "[I 2025-05-14 17:44:20,997] Trial 7 finished with value: 0.6524665697600398 and parameters: {'n_estimators': 100, 'max_depth': 49, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 2 with value: 0.6897486872536919.\n",
      "[I 2025-05-14 17:44:22,114] Trial 8 finished with value: 0.6004034011207975 and parameters: {'n_estimators': 400, 'max_depth': 20, 'min_samples_split': 16, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 2 with value: 0.6897486872536919.\n",
      "[I 2025-05-14 17:44:32,465] Trial 9 finished with value: 0.532041294447856 and parameters: {'n_estimators': 200, 'max_depth': 43, 'min_samples_split': 18, 'min_samples_leaf': 2, 'max_features': None, 'bootstrap': False}. Best is trial 2 with value: 0.6897486872536919.\n",
      "[I 2025-05-14 17:45:03,869] Trial 10 finished with value: 0.5220705973365865 and parameters: {'n_estimators': 700, 'max_depth': 33, 'min_samples_split': 12, 'min_samples_leaf': 10, 'max_features': None, 'bootstrap': False}. Best is trial 2 with value: 0.6897486872536919.\n",
      "[I 2025-05-14 17:45:07,396] Trial 11 finished with value: 0.7285233409297585 and parameters: {'n_estimators': 100, 'max_depth': 49, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': None, 'bootstrap': True}. Best is trial 11 with value: 0.7285233409297585.\n",
      "[I 2025-05-14 17:45:10,823] Trial 12 finished with value: 0.720318798730166 and parameters: {'n_estimators': 100, 'max_depth': 49, 'min_samples_split': 12, 'min_samples_leaf': 4, 'max_features': None, 'bootstrap': True}. Best is trial 11 with value: 0.7285233409297585.\n",
      "[I 2025-05-14 17:45:14,206] Trial 13 finished with value: 0.7262945043385545 and parameters: {'n_estimators': 100, 'max_depth': 49, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': None, 'bootstrap': True}. Best is trial 11 with value: 0.7285233409297585.\n",
      "[I 2025-05-14 17:45:53,011] Trial 14 finished with value: 0.48056397610736046 and parameters: {'n_estimators': 700, 'max_depth': 49, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': None, 'bootstrap': False}. Best is trial 11 with value: 0.7285233409297585.\n",
      "[I 2025-05-14 17:45:54,911] Trial 15 finished with value: 0.511709423567155 and parameters: {'n_estimators': 100, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': None, 'bootstrap': True}. Best is trial 11 with value: 0.7285233409297585.\n",
      "[I 2025-05-14 17:46:04,038] Trial 16 finished with value: 0.7056487725929785 and parameters: {'n_estimators': 300, 'max_depth': 14, 'min_samples_split': 15, 'min_samples_leaf': 3, 'max_features': None, 'bootstrap': True}. Best is trial 11 with value: 0.7285233409297585.\n",
      "[I 2025-05-14 17:46:13,540] Trial 17 finished with value: 0.7190653693422207 and parameters: {'n_estimators': 300, 'max_depth': 16, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None, 'bootstrap': True}. Best is trial 11 with value: 0.7285233409297585.\n",
      "[I 2025-05-14 17:46:52,694] Trial 18 finished with value: 0.5097055886636809 and parameters: {'n_estimators': 700, 'max_depth': 22, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': False}. Best is trial 11 with value: 0.7285233409297585.\n",
      "[I 2025-05-14 17:46:59,832] Trial 19 finished with value: 0.7362911074430163 and parameters: {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': None, 'bootstrap': True}. Best is trial 19 with value: 0.7362911074430163.\n",
      "[I 2025-05-14 17:47:01,268] Trial 20 finished with value: 0.6249192953729779 and parameters: {'n_estimators': 600, 'max_depth': 47, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': True}. Best is trial 19 with value: 0.7362911074430163.\n",
      "[I 2025-05-14 17:47:08,493] Trial 21 finished with value: 0.7362911074430163 and parameters: {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': None, 'bootstrap': True}. Best is trial 19 with value: 0.7362911074430163.\n",
      "[I 2025-05-14 17:47:15,634] Trial 22 finished with value: 0.7362911074430163 and parameters: {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': None, 'bootstrap': True}. Best is trial 19 with value: 0.7362911074430163.\n",
      "[I 2025-05-14 17:47:23,015] Trial 23 finished with value: 0.7362911074430163 and parameters: {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': None, 'bootstrap': True}. Best is trial 19 with value: 0.7362911074430163.\n",
      "[I 2025-05-14 17:47:36,061] Trial 24 finished with value: 0.7255383003261897 and parameters: {'n_estimators': 400, 'max_depth': 46, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None, 'bootstrap': True}. Best is trial 19 with value: 0.7362911074430163.\n",
      "[I 2025-05-14 17:47:42,760] Trial 25 finished with value: 0.7364344498410835 and parameters: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': None, 'bootstrap': True}. Best is trial 25 with value: 0.7364344498410835.\n",
      "[I 2025-05-14 17:47:55,958] Trial 26 finished with value: 0.518862807261351 and parameters: {'n_estimators': 300, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': None, 'bootstrap': False}. Best is trial 25 with value: 0.7364344498410835.\n",
      "[I 2025-05-14 17:48:03,682] Trial 27 finished with value: 0.7397382493400023 and parameters: {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': True}. Best is trial 27 with value: 0.7397382493400023.\n",
      "[I 2025-05-14 17:48:04,657] Trial 28 finished with value: 0.6267729065923152 and parameters: {'n_estimators': 400, 'max_depth': None, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True}. Best is trial 27 with value: 0.7397382493400023.\n",
      "[I 2025-05-14 17:48:21,832] Trial 29 finished with value: 0.516923867530691 and parameters: {'n_estimators': 300, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': False}. Best is trial 27 with value: 0.7397382493400023.\n",
      "[I 2025-05-14 17:48:23,118] Trial 30 finished with value: 0.6205863415645373 and parameters: {'n_estimators': 500, 'max_depth': 36, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': True}. Best is trial 27 with value: 0.7397382493400023.\n",
      "[I 2025-05-14 17:48:30,085] Trial 31 finished with value: 0.7151680135875147 and parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': None, 'bootstrap': True}. Best is trial 27 with value: 0.7397382493400023.\n",
      "[I 2025-05-14 17:49:04,603] Trial 32 finished with value: 0.7383910299744172 and parameters: {'n_estimators': 1000, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': None, 'bootstrap': True}. Best is trial 27 with value: 0.7397382493400023.\n",
      "[I 2025-05-14 17:49:34,546] Trial 33 finished with value: 0.7183295386871429 and parameters: {'n_estimators': 1000, 'max_depth': 42, 'min_samples_split': 3, 'min_samples_leaf': 6, 'max_features': None, 'bootstrap': True}. Best is trial 27 with value: 0.7397382493400023.\n",
      "[I 2025-05-14 17:50:11,360] Trial 34 finished with value: 0.7458211862649413 and parameters: {'n_estimators': 900, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': True}. Best is trial 34 with value: 0.7458211862649413.\n",
      "[I 2025-05-14 17:50:15,173] Trial 35 finished with value: 0.671628051428591 and parameters: {'n_estimators': 900, 'max_depth': 34, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 34 with value: 0.7458211862649413.\n",
      "[I 2025-05-14 17:50:58,566] Trial 36 finished with value: 0.7455459067109683 and parameters: {'n_estimators': 1000, 'max_depth': 29, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': True}. Best is trial 34 with value: 0.7458211862649413.\n",
      "[I 2025-05-14 17:51:43,025] Trial 37 finished with value: 0.7461210225385635 and parameters: {'n_estimators': 1000, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:51:46,727] Trial 38 finished with value: 0.6693391206384582 and parameters: {'n_estimators': 900, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:51:48,920] Trial 39 finished with value: 0.6396531931084892 and parameters: {'n_estimators': 800, 'max_depth': 29, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:51:50,383] Trial 40 finished with value: 0.35355454866474734 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:52:27,941] Trial 41 finished with value: 0.7429538770598243 and parameters: {'n_estimators': 1000, 'max_depth': 21, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': None, 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:53:01,075] Trial 42 finished with value: 0.744939679851002 and parameters: {'n_estimators': 900, 'max_depth': 37, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:53:29,132] Trial 43 finished with value: 0.7432952366277774 and parameters: {'n_estimators': 800, 'max_depth': 37, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': None, 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:53:50,460] Trial 44 finished with value: 0.6767329905045674 and parameters: {'n_estimators': 800, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:54:19,096] Trial 45 finished with value: 0.7432952366277774 and parameters: {'n_estimators': 800, 'max_depth': 37, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None, 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:55:00,262] Trial 46 finished with value: 0.516739810932348 and parameters: {'n_estimators': 900, 'max_depth': 37, 'min_samples_split': 4, 'min_samples_leaf': 9, 'max_features': None, 'bootstrap': False}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:55:32,226] Trial 47 finished with value: 0.7390824688261871 and parameters: {'n_estimators': 900, 'max_depth': 18, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:55:57,758] Trial 48 finished with value: 0.7267516310564632 and parameters: {'n_estimators': 800, 'max_depth': 26, 'min_samples_split': 14, 'min_samples_leaf': 2, 'max_features': None, 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "[I 2025-05-14 17:55:59,596] Trial 49 finished with value: 0.4343802844301502 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'min_samples_split': 20, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 37 with value: 0.7461210225385635.\n",
      "\n",
      "🔥 Best R² Score: 0.7461210225385635\n",
      "🔥 Best Parameters:\n",
      " {'n_estimators': 1000, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Optuna run\n",
    "# Split your preprocessed features and target\n",
    "X_train_ori = pipeline.fit_transform(train_df)\n",
    "y_train_ori = train_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_ori, y_train_ori, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000, step=100),  # base used 100\n",
    "        \"max_depth\": trial.suggest_categorical(\"max_depth\", [None] + list(range(5, 51))),  # base used None\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),  # base used 2\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),    # base used 1\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),  # base unclear, include common\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),  # default is True\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42,\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return r2_score(y_test, preds)\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Show best results\n",
    "print(\"\\n🔥 Best R² Score:\", study.best_value)\n",
    "print(\"🔥 Best Parameters:\\n\", study.best_params)\n",
    "\n",
    "    # \"RandomForest\": RandomForestRegressor(\n",
    "    #     n_estimators=100,\n",
    "    #     max_depth=None,\n",
    "    #     min_samples_split=2,\n",
    "    #     min_samples_leaf=1,\n",
    "    #     n_jobs=-1,\n",
    "    #     random_state=42,\n",
    "    #     verbose=0\n",
    "    # ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c15cf86-c902-4907-8baa-d765d148e37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 18:01:04,863] A new study created in memory with name: no-name-19c80d9a-c587-40c4-8442-143ae5db4820\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687a449ad808441c98342ade4cff9766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 18:01:17,437] Trial 11 finished with value: 0.5975911518758866 and parameters: {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.0302178803345446, 'num_leaves': 1721, 'min_child_samples': 15, 'subsample': 0.850569966679041, 'colsample_bytree': 0.9822952605646738, 'reg_alpha': 6.0315370079391, 'reg_lambda': 3.5470412587262743}. Best is trial 11 with value: 0.5975911518758866.\n",
      "[I 2025-05-14 18:01:34,410] Trial 9 finished with value: 0.7597649823757288 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.05306215383786456, 'num_leaves': 2926, 'min_child_samples': 92, 'subsample': 0.643628859197829, 'colsample_bytree': 0.6507275558473318, 'reg_alpha': 8.66196021878192, 'reg_lambda': 4.20424289204817}. Best is trial 9 with value: 0.7597649823757288.\n",
      "[I 2025-05-14 18:02:27,099] Trial 12 finished with value: 0.83794916686368 and parameters: {'n_estimators': 1300, 'max_depth': 7, 'learning_rate': 0.2188314494010197, 'num_leaves': 238, 'min_child_samples': 64, 'subsample': 0.9426406214308116, 'colsample_bytree': 0.9957045662004209, 'reg_alpha': 5.986826108619337, 'reg_lambda': 2.928554093675444}. Best is trial 12 with value: 0.83794916686368.\n",
      "[I 2025-05-14 18:02:58,481] Trial 2 finished with value: 0.831347734475509 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.05045210785431767, 'num_leaves': 1547, 'min_child_samples': 81, 'subsample': 0.6889613884301831, 'colsample_bytree': 0.6082232678792447, 'reg_alpha': 8.740116922923818, 'reg_lambda': 3.285165073172994}. Best is trial 12 with value: 0.83794916686368.\n",
      "[I 2025-05-14 18:03:03,260] Trial 14 finished with value: 0.7599150768177074 and parameters: {'n_estimators': 3400, 'max_depth': 3, 'learning_rate': 0.01381412440200094, 'num_leaves': 2905, 'min_child_samples': 70, 'subsample': 0.9147737720769658, 'colsample_bytree': 0.6502117716205316, 'reg_alpha': 3.6141974646776487, 'reg_lambda': 4.015849033802825}. Best is trial 12 with value: 0.83794916686368.\n",
      "[I 2025-05-14 18:03:35,288] Trial 1 finished with value: 0.8438331207530391 and parameters: {'n_estimators': 2000, 'max_depth': 8, 'learning_rate': 0.15768663147208217, 'num_leaves': 1246, 'min_child_samples': 73, 'subsample': 0.7830106699873722, 'colsample_bytree': 0.572609517889008, 'reg_alpha': 7.661733276554959, 'reg_lambda': 6.184430381423507}. Best is trial 1 with value: 0.8438331207530391.\n",
      "[I 2025-05-14 18:03:48,294] Trial 7 finished with value: 0.812154114344313 and parameters: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.021049657474171445, 'num_leaves': 2736, 'min_child_samples': 7, 'subsample': 0.8164456643103162, 'colsample_bytree': 0.6880938250425128, 'reg_alpha': 3.6065539344854205, 'reg_lambda': 2.6246795328450157}. Best is trial 1 with value: 0.8438331207530391.\n",
      "[I 2025-05-14 18:04:53,497] Trial 16 finished with value: 0.8533672877701861 and parameters: {'n_estimators': 3800, 'max_depth': 6, 'learning_rate': 0.1514197228544641, 'num_leaves': 1951, 'min_child_samples': 77, 'subsample': 0.9330512803485513, 'colsample_bytree': 0.9081753323564226, 'reg_alpha': 6.506803494940449, 'reg_lambda': 3.855766980641727}. Best is trial 16 with value: 0.8533672877701861.\n",
      "[I 2025-05-14 18:04:55,511] Trial 10 finished with value: 0.8593662722572708 and parameters: {'n_estimators': 3800, 'max_depth': 7, 'learning_rate': 0.072731268815327, 'num_leaves': 237, 'min_child_samples': 89, 'subsample': 0.5276132173571262, 'colsample_bytree': 0.9709273967473571, 'reg_alpha': 4.494817457977192, 'reg_lambda': 0.6679239019997985}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:05:11,080] Trial 13 finished with value: 0.8434743124203444 and parameters: {'n_estimators': 1200, 'max_depth': 8, 'learning_rate': 0.07010177314616386, 'num_leaves': 1924, 'min_child_samples': 9, 'subsample': 0.7901839243101372, 'colsample_bytree': 0.6520147314924365, 'reg_alpha': 9.396423083854845, 'reg_lambda': 0.5975353930535343}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:05:17,668] Trial 19 finished with value: 0.7577980997100524 and parameters: {'n_estimators': 3500, 'max_depth': 3, 'learning_rate': 0.01332692610976331, 'num_leaves': 880, 'min_child_samples': 61, 'subsample': 0.9780793069153499, 'colsample_bytree': 0.5574392896149974, 'reg_alpha': 5.687460949967703, 'reg_lambda': 6.342741610523712}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:05:21,600] Trial 22 finished with value: 0.8173288775819585 and parameters: {'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.11834646017712783, 'num_leaves': 916, 'min_child_samples': 41, 'subsample': 0.7940341347169129, 'colsample_bytree': 0.8225678278523265, 'reg_alpha': 0.9055571128953432, 'reg_lambda': 4.3053443454862075}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:05:44,556] Trial 4 finished with value: 0.8574070090277073 and parameters: {'n_estimators': 2100, 'max_depth': 9, 'learning_rate': 0.06028626186085645, 'num_leaves': 2147, 'min_child_samples': 96, 'subsample': 0.5355249804520172, 'colsample_bytree': 0.7517061533559108, 'reg_alpha': 7.508394221268886, 'reg_lambda': 6.248446554922297}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:08:11,042] Trial 17 finished with value: 0.8540861603943662 and parameters: {'n_estimators': 1500, 'max_depth': 9, 'learning_rate': 0.038507484168141023, 'num_leaves': 1072, 'min_child_samples': 74, 'subsample': 0.8214306249861018, 'colsample_bytree': 0.6417180462381218, 'reg_alpha': 0.9587009844383965, 'reg_lambda': 6.361473858186994}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:08:41,194] Trial 21 finished with value: 0.833445120592291 and parameters: {'n_estimators': 2800, 'max_depth': 11, 'learning_rate': 0.08290098245791415, 'num_leaves': 285, 'min_child_samples': 13, 'subsample': 0.9429928761407428, 'colsample_bytree': 0.8229287674472006, 'reg_alpha': 8.69622807621435, 'reg_lambda': 0.05617853331384759}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:08:47,062] Trial 25 finished with value: 0.8586191100222315 and parameters: {'n_estimators': 2800, 'max_depth': 11, 'learning_rate': 0.094247082439595, 'num_leaves': 75, 'min_child_samples': 99, 'subsample': 0.5497959192157164, 'colsample_bytree': 0.8767809129777691, 'reg_alpha': 3.974818547822337, 'reg_lambda': 9.431966449907822}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:08:47,595] Trial 6 finished with value: 0.8387984289814836 and parameters: {'n_estimators': 2000, 'max_depth': 7, 'learning_rate': 0.014166683157941156, 'num_leaves': 2595, 'min_child_samples': 22, 'subsample': 0.9752344714023577, 'colsample_bytree': 0.8333387992577956, 'reg_alpha': 0.976488829368044, 'reg_lambda': 4.840789733857668}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:08:49,544] Trial 8 finished with value: 0.8482551814797368 and parameters: {'n_estimators': 3300, 'max_depth': 10, 'learning_rate': 0.030574173620496366, 'num_leaves': 237, 'min_child_samples': 34, 'subsample': 0.839630727089087, 'colsample_bytree': 0.6526942548708818, 'reg_alpha': 9.387843295962856, 'reg_lambda': 3.086409019634403}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:09:42,994] Trial 24 finished with value: 0.8578777173242432 and parameters: {'n_estimators': 4000, 'max_depth': 11, 'learning_rate': 0.11576332007484023, 'num_leaves': 119, 'min_child_samples': 97, 'subsample': 0.54000795674518, 'colsample_bytree': 0.8458833084830829, 'reg_alpha': 1.0943847863980025, 'reg_lambda': 9.88226463334797}. Best is trial 10 with value: 0.8593662722572708.\n",
      "[I 2025-05-14 18:09:54,202] Trial 26 finished with value: 0.8597767245127322 and parameters: {'n_estimators': 2500, 'max_depth': 11, 'learning_rate': 0.08538707131470794, 'num_leaves': 135, 'min_child_samples': 98, 'subsample': 0.5138410930316747, 'colsample_bytree': 0.8087819727679879, 'reg_alpha': 3.198134982017156, 'reg_lambda': 9.65657919393731}. Best is trial 26 with value: 0.8597767245127322.\n",
      "[I 2025-05-14 18:09:59,239] Trial 20 finished with value: 0.8469754316543431 and parameters: {'n_estimators': 2700, 'max_depth': 10, 'learning_rate': 0.06600867386183458, 'num_leaves': 619, 'min_child_samples': 24, 'subsample': 0.6937325513734846, 'colsample_bytree': 0.9799222377721575, 'reg_alpha': 2.6899924714334125, 'reg_lambda': 7.820561402927724}. Best is trial 26 with value: 0.8597767245127322.\n",
      "[I 2025-05-14 18:10:21,477] Trial 30 finished with value: 0.8352843073256344 and parameters: {'n_estimators': 2800, 'max_depth': 12, 'learning_rate': 0.2904819934103704, 'num_leaves': 35, 'min_child_samples': 100, 'subsample': 0.5011485156256889, 'colsample_bytree': 0.9117681516279671, 'reg_alpha': 3.94663992531167, 'reg_lambda': 9.842443843283313}. Best is trial 26 with value: 0.8597767245127322.\n",
      "[I 2025-05-14 18:10:56,353] Trial 23 finished with value: 0.8603218298620644 and parameters: {'n_estimators': 3000, 'max_depth': 11, 'learning_rate': 0.09216865202064772, 'num_leaves': 28, 'min_child_samples': 40, 'subsample': 0.5276614407170144, 'colsample_bytree': 0.8239904218170874, 'reg_alpha': 0.0787097082876258, 'reg_lambda': 9.831250240403401}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:11:33,954] Trial 32 finished with value: 0.8369437006063793 and parameters: {'n_estimators': 2700, 'max_depth': 12, 'learning_rate': 0.234321159397959, 'num_leaves': 538, 'min_child_samples': 85, 'subsample': 0.6006953876657708, 'colsample_bytree': 0.916041180972426, 'reg_alpha': 4.138859064632199, 'reg_lambda': 8.608837827623073}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:11:47,581] Trial 33 finished with value: 0.8315346656793917 and parameters: {'n_estimators': 2700, 'max_depth': 12, 'learning_rate': 0.26434912914333675, 'num_leaves': 459, 'min_child_samples': 85, 'subsample': 0.5965788507269488, 'colsample_bytree': 0.9360657583608295, 'reg_alpha': 2.7729433480586794, 'reg_lambda': 8.181784124855167}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:12:11,232] Trial 27 finished with value: 0.8574326491058134 and parameters: {'n_estimators': 2800, 'max_depth': 12, 'learning_rate': 0.083831540779744, 'num_leaves': 156, 'min_child_samples': 96, 'subsample': 0.5009239104089542, 'colsample_bytree': 0.7733317731504099, 'reg_alpha': 3.7972964049834075, 'reg_lambda': 9.472798885533226}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:12:19,762] Trial 29 finished with value: 0.8577126253630291 and parameters: {'n_estimators': 2800, 'max_depth': 11, 'learning_rate': 0.09372332071872212, 'num_leaves': 67, 'min_child_samples': 100, 'subsample': 0.5012809565517617, 'colsample_bytree': 0.908060271208492, 'reg_alpha': 3.8252983797414055, 'reg_lambda': 9.803198571430451}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:12:20,335] Trial 31 finished with value: 0.8550736646210523 and parameters: {'n_estimators': 2800, 'max_depth': 12, 'learning_rate': 0.09792798432594102, 'num_leaves': 472, 'min_child_samples': 86, 'subsample': 0.5021084737679455, 'colsample_bytree': 0.9263696249203864, 'reg_alpha': 3.8125425003675044, 'reg_lambda': 9.987217865370743}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:12:53,420] Trial 0 finished with value: 0.8516586897954869 and parameters: {'n_estimators': 2100, 'max_depth': 9, 'learning_rate': 0.02295794498054022, 'num_leaves': 1975, 'min_child_samples': 45, 'subsample': 0.6853088376708845, 'colsample_bytree': 0.7068718271447566, 'reg_alpha': 8.327644005601256, 'reg_lambda': 3.274098278299238}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:13:25,379] Trial 34 finished with value: 0.8560171795670521 and parameters: {'n_estimators': 2700, 'max_depth': 12, 'learning_rate': 0.09318238723477235, 'num_leaves': 554, 'min_child_samples': 89, 'subsample': 0.6037373637140284, 'colsample_bytree': 0.9084903768965671, 'reg_alpha': 4.412604082071105, 'reg_lambda': 9.799380666145105}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:13:39,139] Trial 28 finished with value: 0.8561080755942267 and parameters: {'n_estimators': 2700, 'max_depth': 10, 'learning_rate': 0.08795295386409964, 'num_leaves': 2329, 'min_child_samples': 98, 'subsample': 0.5194904404479787, 'colsample_bytree': 0.5072587152785111, 'reg_alpha': 3.982515339755369, 'reg_lambda': 9.48721834963086}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:14:18,482] Trial 35 finished with value: 0.8549398535132244 and parameters: {'n_estimators': 2500, 'max_depth': 12, 'learning_rate': 0.09694601484905607, 'num_leaves': 622, 'min_child_samples': 87, 'subsample': 0.5970753290324753, 'colsample_bytree': 0.9236680006717335, 'reg_alpha': 2.6062970082371906, 'reg_lambda': 8.564190669984917}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:15:00,357] Trial 42 finished with value: 0.8442539300054497 and parameters: {'n_estimators': 2400, 'max_depth': 10, 'learning_rate': 0.17030731476247438, 'num_leaves': 680, 'min_child_samples': 53, 'subsample': 0.6031711485846484, 'colsample_bytree': 0.7902596900692731, 'reg_alpha': 4.898028183291263, 'reg_lambda': 1.2703384914550906}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:15:06,485] Trial 18 finished with value: 0.8564517759407655 and parameters: {'n_estimators': 2400, 'max_depth': 12, 'learning_rate': 0.016493639047829287, 'num_leaves': 2817, 'min_child_samples': 86, 'subsample': 0.9614256950565507, 'colsample_bytree': 0.6543640668900637, 'reg_alpha': 3.2491414294239296, 'reg_lambda': 3.9899424560917818}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:15:33,107] Trial 36 finished with value: 0.8506625384360502 and parameters: {'n_estimators': 3100, 'max_depth': 12, 'learning_rate': 0.09201242546533478, 'num_leaves': 496, 'min_child_samples': 52, 'subsample': 0.6042150918624476, 'colsample_bytree': 0.7654769096878166, 'reg_alpha': 2.5414749534805727, 'reg_lambda': 8.237751441021866}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:15:44,902] Trial 40 finished with value: 0.839479581578762 and parameters: {'n_estimators': 3200, 'max_depth': 9, 'learning_rate': 0.16715118621358954, 'num_leaves': 718, 'min_child_samples': 49, 'subsample': 0.5931536801336845, 'colsample_bytree': 0.5097686461310764, 'reg_alpha': 2.1923264438949315, 'reg_lambda': 1.5296294161688309}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:15:48,300] Trial 3 finished with value: 0.8543554831446356 and parameters: {'n_estimators': 3000, 'max_depth': 10, 'learning_rate': 0.016906457800218624, 'num_leaves': 1951, 'min_child_samples': 75, 'subsample': 0.7415800769124311, 'colsample_bytree': 0.710088358937524, 'reg_alpha': 8.461524103269555, 'reg_lambda': 0.2094018982021728}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:15:58,533] Trial 37 finished with value: 0.8551466042797415 and parameters: {'n_estimators': 3200, 'max_depth': 10, 'learning_rate': 0.09619631689603075, 'num_leaves': 580, 'min_child_samples': 51, 'subsample': 0.5893933003410058, 'colsample_bytree': 0.7750297591979836, 'reg_alpha': 2.2240561041050997, 'reg_lambda': 7.724354391960552}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:16:08,405] Trial 39 finished with value: 0.8486560026288791 and parameters: {'n_estimators': 3200, 'max_depth': 10, 'learning_rate': 0.11212011443238303, 'num_leaves': 690, 'min_child_samples': 52, 'subsample': 0.5953261904770379, 'colsample_bytree': 0.7158545705991505, 'reg_alpha': 1.9533972628206102, 'reg_lambda': 1.5608813006937496}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:16:15,897] Trial 38 finished with value: 0.8511747012042177 and parameters: {'n_estimators': 3100, 'max_depth': 10, 'learning_rate': 0.09201154640830464, 'num_leaves': 672, 'min_child_samples': 47, 'subsample': 0.5020575585845811, 'colsample_bytree': 0.7770233577950394, 'reg_alpha': 1.9948382718497482, 'reg_lambda': 2.041480429801779}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:16:22,359] Trial 45 finished with value: 0.8429998837132257 and parameters: {'n_estimators': 2300, 'max_depth': 10, 'learning_rate': 0.15942888724316295, 'num_leaves': 370, 'min_child_samples': 52, 'subsample': 0.5570555496841356, 'colsample_bytree': 0.8642711438524106, 'reg_alpha': 4.946414952664422, 'reg_lambda': 1.3974337619954538}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:16:33,449] Trial 44 finished with value: 0.8423603026232513 and parameters: {'n_estimators': 3100, 'max_depth': 10, 'learning_rate': 0.14702468310347216, 'num_leaves': 371, 'min_child_samples': 52, 'subsample': 0.5699559679245234, 'colsample_bytree': 0.8598238996147658, 'reg_alpha': 1.8693918559856328, 'reg_lambda': 1.835047733685614}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:17:10,696] Trial 5 finished with value: 0.848675806997588 and parameters: {'n_estimators': 1800, 'max_depth': 12, 'learning_rate': 0.015397079297339775, 'num_leaves': 648, 'min_child_samples': 31, 'subsample': 0.99245437579738, 'colsample_bytree': 0.6635296485816524, 'reg_alpha': 2.0479822618914953, 'reg_lambda': 0.5901842421571857}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:17:26,674] Trial 43 finished with value: 0.8397836483827765 and parameters: {'n_estimators': 2400, 'max_depth': 10, 'learning_rate': 0.16649904290105907, 'num_leaves': 689, 'min_child_samples': 49, 'subsample': 0.7257135913529748, 'colsample_bytree': 0.5014965306596493, 'reg_alpha': 0.19852487735022706, 'reg_lambda': 1.5255784947678936}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:18:10,613] Trial 48 finished with value: 0.8439918370118542 and parameters: {'n_estimators': 3100, 'max_depth': 9, 'learning_rate': 0.12586694640392848, 'num_leaves': 1375, 'min_child_samples': 36, 'subsample': 0.5605987830624729, 'colsample_bytree': 0.8644419233100292, 'reg_alpha': 1.7499464416149588, 'reg_lambda': 7.231865442939537}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:18:25,440] Trial 49 finished with value: 0.8536272634067374 and parameters: {'n_estimators': 1700, 'max_depth': 8, 'learning_rate': 0.0430007687942748, 'num_leaves': 332, 'min_child_samples': 42, 'subsample': 0.5563507403328647, 'colsample_bytree': 0.8699181068368051, 'reg_alpha': 0.31296372379585286, 'reg_lambda': 6.842202113680716}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:18:51,179] Trial 41 finished with value: 0.8539641525674766 and parameters: {'n_estimators': 3600, 'max_depth': 9, 'learning_rate': 0.04243242299432155, 'num_leaves': 824, 'min_child_samples': 49, 'subsample': 0.598029685967961, 'colsample_bytree': 0.5009402105500951, 'reg_alpha': 2.114131614331774, 'reg_lambda': 7.370629308309681}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:18:57,424] Trial 15 finished with value: 0.8469974603574518 and parameters: {'n_estimators': 3800, 'max_depth': 11, 'learning_rate': 0.036893221901260806, 'num_leaves': 1434, 'min_child_samples': 24, 'subsample': 0.6388842157846664, 'colsample_bytree': 0.6038998368180164, 'reg_alpha': 0.4541973182631698, 'reg_lambda': 5.00400605750849}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:19:18,931] Trial 46 finished with value: 0.8548796329990452 and parameters: {'n_estimators': 3100, 'max_depth': 11, 'learning_rate': 0.045742291378618946, 'num_leaves': 1337, 'min_child_samples': 49, 'subsample': 0.5562210524192599, 'colsample_bytree': 0.8684499798298112, 'reg_alpha': 2.1181043840211924, 'reg_lambda': 7.300955273381445}. Best is trial 23 with value: 0.8603218298620644.\n",
      "[I 2025-05-14 18:20:03,306] Trial 47 finished with value: 0.8515160508472899 and parameters: {'n_estimators': 3300, 'max_depth': 9, 'learning_rate': 0.044503197370577244, 'num_leaves': 1343, 'min_child_samples': 31, 'subsample': 0.5457343394574407, 'colsample_bytree': 0.8648493030120905, 'reg_alpha': 0.17480580251473385, 'reg_lambda': 7.398620411742248}. Best is trial 23 with value: 0.8603218298620644.\n",
      "\n",
      "🔥 Best R² Score: 0.8603218298620644\n",
      "🔥 Best Parameters:\n",
      " {'n_estimators': 3000, 'max_depth': 11, 'learning_rate': 0.09216865202064772, 'num_leaves': 28, 'min_child_samples': 40, 'subsample': 0.5276614407170144, 'colsample_bytree': 0.8239904218170874, 'reg_alpha': 0.0787097082876258, 'reg_lambda': 9.831250240403401}\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Optuna run\n",
    "# Your preprocessed features\n",
    "X_train_ori = pipeline.fit_transform(train_df)\n",
    "y_train_ori = train_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_ori, y_train_ori, test_size=0.2, random_state=42)\n",
    "X_train, X_test, cat_features = preprocess_for_model('LightGBM', X_train, X_test)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 4000, step=100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 10.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 10.0),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        'verbose' : -1\n",
    "    }\n",
    "\n",
    "\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train, categorical_feature=cat_features)\n",
    "    preds = model.predict(X_test)\n",
    "    return r2_score(y_test, preds)\n",
    "\n",
    "# Run the Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True, n_jobs=-1)\n",
    "\n",
    "# Print best results\n",
    "print(\"\\n🔥 Best R² Score:\", study.best_value)\n",
    "print(\"🔥 Best Parameters:\\n\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b1423b8-9311-4077-b8fc-7d3c9668cf13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 18:20:03,895] A new study created in memory with name: no-name-75eccbc9-8bcf-4735-aa63-d4e77f0da50e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c7dee2a64a4cbaa7ff1e86c773e15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 18:20:07,978] Trial 0 finished with value: 0.8439252659323679 and parameters: {'n_estimators': 1300, 'max_depth': 5, 'learning_rate': 0.0393664371123296, 'subsample': 0.7976482143187783, 'colsample_bytree': 0.9422857783464369, 'reg_alpha': 3.5592832944056396, 'reg_lambda': 7.683368484059959}. Best is trial 0 with value: 0.8439252659323679.\n",
      "[I 2025-05-14 18:20:13,403] Trial 1 finished with value: 0.7978600520393005 and parameters: {'n_estimators': 3000, 'max_depth': 3, 'learning_rate': 0.024629726414522504, 'subsample': 0.6509878254936378, 'colsample_bytree': 0.890905223776405, 'reg_alpha': 2.925918508026358, 'reg_lambda': 5.841377085962448}. Best is trial 0 with value: 0.8439252659323679.\n",
      "[I 2025-05-14 18:20:24,246] Trial 2 finished with value: 0.8368628593357522 and parameters: {'n_estimators': 1600, 'max_depth': 10, 'learning_rate': 0.08929134248623896, 'subsample': 0.6159577056560219, 'colsample_bytree': 0.9257683148637802, 'reg_alpha': 1.6839537342751554, 'reg_lambda': 4.168625045257527}. Best is trial 0 with value: 0.8439252659323679.\n",
      "[I 2025-05-14 18:20:35,152] Trial 3 finished with value: 0.8367137339664196 and parameters: {'n_estimators': 2200, 'max_depth': 9, 'learning_rate': 0.07016183027326502, 'subsample': 0.8637673805769971, 'colsample_bytree': 0.6680813078210188, 'reg_alpha': 6.465097288284154, 'reg_lambda': 2.4505171941413373}. Best is trial 0 with value: 0.8439252659323679.\n",
      "[I 2025-05-14 18:20:53,372] Trial 4 finished with value: 0.844617162594653 and parameters: {'n_estimators': 3100, 'max_depth': 9, 'learning_rate': 0.03298665507556837, 'subsample': 0.7720904060516542, 'colsample_bytree': 0.8557905089624691, 'reg_alpha': 6.2623667520683055, 'reg_lambda': 9.48896442886936}. Best is trial 4 with value: 0.844617162594653.\n",
      "[I 2025-05-14 18:20:54,259] Trial 5 finished with value: 0.6191811117570336 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01206363153349792, 'subsample': 0.7414952808621453, 'colsample_bytree': 0.6209716101026073, 'reg_alpha': 0.6189186807693214, 'reg_lambda': 2.1831989664851505}. Best is trial 4 with value: 0.844617162594653.\n",
      "[I 2025-05-14 18:20:56,356] Trial 6 finished with value: 0.8371277615507428 and parameters: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.11380928502094688, 'subsample': 0.9280555951933851, 'colsample_bytree': 0.9363143830724667, 'reg_alpha': 6.457599393762228, 'reg_lambda': 9.510126417715144}. Best is trial 4 with value: 0.844617162594653.\n",
      "[I 2025-05-14 18:21:03,505] Trial 7 finished with value: 0.8298006343861675 and parameters: {'n_estimators': 2200, 'max_depth': 6, 'learning_rate': 0.1962606609553981, 'subsample': 0.8476024536725782, 'colsample_bytree': 0.6591346121540097, 'reg_alpha': 4.86763426980465, 'reg_lambda': 0.7271889327722203}. Best is trial 4 with value: 0.844617162594653.\n",
      "[I 2025-05-14 18:21:11,789] Trial 8 finished with value: 0.8480056657346724 and parameters: {'n_estimators': 2100, 'max_depth': 6, 'learning_rate': 0.027059525660105506, 'subsample': 0.9036614222550898, 'colsample_bytree': 0.935965540158222, 'reg_alpha': 6.669842625935462, 'reg_lambda': 9.288164406144974}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:21:16,121] Trial 9 finished with value: 0.8166622237840924 and parameters: {'n_estimators': 400, 'max_depth': 10, 'learning_rate': 0.019372443059611385, 'subsample': 0.8336607216703714, 'colsample_bytree': 0.8051773347812468, 'reg_alpha': 4.5455929007828715, 'reg_lambda': 1.4594789422868726}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:21:48,340] Trial 10 finished with value: 0.8193718225849703 and parameters: {'n_estimators': 3600, 'max_depth': 12, 'learning_rate': 0.010013687665410475, 'subsample': 0.9940371396223736, 'colsample_bytree': 0.9902467385164229, 'reg_alpha': 9.975412442948363, 'reg_lambda': 6.974544874888109}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:22:04,087] Trial 11 finished with value: 0.8445386589497205 and parameters: {'n_estimators': 3100, 'max_depth': 8, 'learning_rate': 0.03647649919439301, 'subsample': 0.7455858195485034, 'colsample_bytree': 0.8200444215670281, 'reg_alpha': 8.083712167253728, 'reg_lambda': 9.583552831352486}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:22:26,292] Trial 12 finished with value: 0.8290029600432904 and parameters: {'n_estimators': 2700, 'max_depth': 12, 'learning_rate': 0.0229521055372994, 'subsample': 0.9295660788816082, 'colsample_bytree': 0.8554175744798359, 'reg_alpha': 7.027613713514677, 'reg_lambda': 8.146864409280724}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:22:33,218] Trial 13 finished with value: 0.823800974384862 and parameters: {'n_estimators': 3900, 'max_depth': 3, 'learning_rate': 0.0428914030821425, 'subsample': 0.724381270112201, 'colsample_bytree': 0.7492926869770136, 'reg_alpha': 8.384974666899385, 'reg_lambda': 9.964210998735183}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:22:41,593] Trial 14 finished with value: 0.8451308565501066 and parameters: {'n_estimators': 1700, 'max_depth': 7, 'learning_rate': 0.01963975657655058, 'subsample': 0.8901610066760995, 'colsample_bytree': 0.7681281758577745, 'reg_alpha': 5.623073015444381, 'reg_lambda': 5.511663443766034}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:22:46,871] Trial 15 finished with value: 0.8254291379223457 and parameters: {'n_estimators': 1400, 'max_depth': 6, 'learning_rate': 0.015004997513372007, 'subsample': 0.9106544153501575, 'colsample_bytree': 0.7443264352771574, 'reg_alpha': 5.307461924091992, 'reg_lambda': 4.081246356777282}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:22:52,208] Trial 16 finished with value: 0.8090963399918147 and parameters: {'n_estimators': 1800, 'max_depth': 5, 'learning_rate': 0.017022406978561472, 'subsample': 0.9963446631573252, 'colsample_bytree': 0.7472259412058917, 'reg_alpha': 8.397414385620284, 'reg_lambda': 5.7491261396631295}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:22:56,781] Trial 17 finished with value: 0.8395050250561817 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.05993864596206323, 'subsample': 0.8906544637666691, 'colsample_bytree': 0.7068839084127883, 'reg_alpha': 9.963737670633517, 'reg_lambda': 6.764531386507256}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:23:03,025] Trial 18 finished with value: 0.8300794998402278 and parameters: {'n_estimators': 2600, 'max_depth': 4, 'learning_rate': 0.02446407926073011, 'subsample': 0.9628417678621128, 'colsample_bytree': 0.9958608332381641, 'reg_alpha': 3.2056916238204294, 'reg_lambda': 3.6241797088091134}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:23:06,958] Trial 19 finished with value: 0.8198397738841859 and parameters: {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.2614192902860367, 'subsample': 0.8765470437666442, 'colsample_bytree': 0.7820137623165335, 'reg_alpha': 7.43603400734573, 'reg_lambda': 8.324436072158376}. Best is trial 8 with value: 0.8480056657346724.\n",
      "[I 2025-05-14 18:23:14,647] Trial 20 finished with value: 0.8492136696319497 and parameters: {'n_estimators': 2000, 'max_depth': 6, 'learning_rate': 0.0286406475868037, 'subsample': 0.8162894714014907, 'colsample_bytree': 0.8440011083025999, 'reg_alpha': 5.563286679597116, 'reg_lambda': 5.0876646170523}. Best is trial 20 with value: 0.8492136696319497.\n",
      "[I 2025-05-14 18:23:22,512] Trial 21 finished with value: 0.8494926342337745 and parameters: {'n_estimators': 2000, 'max_depth': 6, 'learning_rate': 0.027004519330470936, 'subsample': 0.8192959303692974, 'colsample_bytree': 0.8570989335233169, 'reg_alpha': 5.455742270285657, 'reg_lambda': 5.1926246224207295}. Best is trial 21 with value: 0.8494926342337745.\n",
      "[I 2025-05-14 18:23:30,345] Trial 22 finished with value: 0.8508180263044791 and parameters: {'n_estimators': 2000, 'max_depth': 6, 'learning_rate': 0.03076807008325071, 'subsample': 0.7928281400272622, 'colsample_bytree': 0.8771901156518693, 'reg_alpha': 5.594929370253331, 'reg_lambda': 4.921851561949548}. Best is trial 22 with value: 0.8508180263044791.\n",
      "[I 2025-05-14 18:23:36,287] Trial 23 finished with value: 0.8500043449943744 and parameters: {'n_estimators': 2500, 'max_depth': 4, 'learning_rate': 0.04868327157582338, 'subsample': 0.8118112611569143, 'colsample_bytree': 0.8513306896663906, 'reg_alpha': 4.265138849234921, 'reg_lambda': 3.1075213036425953}. Best is trial 22 with value: 0.8508180263044791.\n",
      "[I 2025-05-14 18:23:42,276] Trial 24 finished with value: 0.8545390452152182 and parameters: {'n_estimators': 2500, 'max_depth': 4, 'learning_rate': 0.048138690796576684, 'subsample': 0.7104773848511483, 'colsample_bytree': 0.8852025940504804, 'reg_alpha': 4.009459803936202, 'reg_lambda': 3.178668986556702}. Best is trial 24 with value: 0.8545390452152182.\n",
      "[I 2025-05-14 18:23:48,277] Trial 25 finished with value: 0.8532604504155731 and parameters: {'n_estimators': 2500, 'max_depth': 4, 'learning_rate': 0.04935878424567608, 'subsample': 0.6891397854709157, 'colsample_bytree': 0.8908842035987696, 'reg_alpha': 4.149071408975391, 'reg_lambda': 3.0891787356291958}. Best is trial 24 with value: 0.8545390452152182.\n",
      "[I 2025-05-14 18:23:56,421] Trial 26 finished with value: 0.8587073416544161 and parameters: {'n_estimators': 3400, 'max_depth': 4, 'learning_rate': 0.1153926798998273, 'subsample': 0.6913046909264277, 'colsample_bytree': 0.9012696734277681, 'reg_alpha': 2.3510784878716744, 'reg_lambda': 0.2974341142491608}. Best is trial 26 with value: 0.8587073416544161.\n",
      "[I 2025-05-14 18:24:05,124] Trial 27 finished with value: 0.8533043935916871 and parameters: {'n_estimators': 3600, 'max_depth': 4, 'learning_rate': 0.13777041874501436, 'subsample': 0.6883344193530154, 'colsample_bytree': 0.9669420837031559, 'reg_alpha': 2.1506583710654095, 'reg_lambda': 0.5269274830303123}. Best is trial 26 with value: 0.8587073416544161.\n",
      "[I 2025-05-14 18:24:11,450] Trial 28 finished with value: 0.8445200297480849 and parameters: {'n_estimators': 3400, 'max_depth': 3, 'learning_rate': 0.13740908885915828, 'subsample': 0.6872382893588865, 'colsample_bytree': 0.9703312693209092, 'reg_alpha': 2.2028255273219437, 'reg_lambda': 0.47896344976653277}. Best is trial 26 with value: 0.8587073416544161.\n",
      "[I 2025-05-14 18:24:23,803] Trial 29 finished with value: 0.8423799049567293 and parameters: {'n_estimators': 4000, 'max_depth': 5, 'learning_rate': 0.15335250094811223, 'subsample': 0.6900146318829874, 'colsample_bytree': 0.9167049963144368, 'reg_alpha': 0.0408375106710932, 'reg_lambda': 0.10404456179126387}. Best is trial 26 with value: 0.8587073416544161.\n",
      "[I 2025-05-14 18:24:32,602] Trial 30 finished with value: 0.8591489585681186 and parameters: {'n_estimators': 3600, 'max_depth': 4, 'learning_rate': 0.09291213185637824, 'subsample': 0.6071138288641564, 'colsample_bytree': 0.9650349937489566, 'reg_alpha': 1.6207337780232343, 'reg_lambda': 1.2556390973710843}. Best is trial 30 with value: 0.8591489585681186.\n",
      "[I 2025-05-14 18:24:41,397] Trial 31 finished with value: 0.8650734897442633 and parameters: {'n_estimators': 3600, 'max_depth': 4, 'learning_rate': 0.08684556600995234, 'subsample': 0.6013049060278864, 'colsample_bytree': 0.9637915732803236, 'reg_alpha': 1.6811734036349635, 'reg_lambda': 1.2172295337024397}. Best is trial 31 with value: 0.8650734897442633.\n",
      "[I 2025-05-14 18:24:47,695] Trial 32 finished with value: 0.8465917481912988 and parameters: {'n_estimators': 3400, 'max_depth': 3, 'learning_rate': 0.08599540337416575, 'subsample': 0.6078913822100099, 'colsample_bytree': 0.9613345251078153, 'reg_alpha': 1.626351935640339, 'reg_lambda': 1.4262066934498039}. Best is trial 31 with value: 0.8650734897442633.\n",
      "[I 2025-05-14 18:24:54,727] Trial 33 finished with value: 0.8587678126157637 and parameters: {'n_estimators': 2900, 'max_depth': 4, 'learning_rate': 0.09558291711576508, 'subsample': 0.6461335809628265, 'colsample_bytree': 0.9114620558373678, 'reg_alpha': 1.0704488828872472, 'reg_lambda': 1.6243488083533577}. Best is trial 31 with value: 0.8650734897442633.\n",
      "[I 2025-05-14 18:24:59,948] Trial 34 finished with value: 0.8430265852415579 and parameters: {'n_estimators': 2900, 'max_depth': 3, 'learning_rate': 0.07990250114871776, 'subsample': 0.6411337913018517, 'colsample_bytree': 0.920692604351029, 'reg_alpha': 0.9890915380465859, 'reg_lambda': 1.4836577169555716}. Best is trial 31 with value: 0.8650734897442633.\n",
      "[I 2025-05-14 18:25:11,178] Trial 35 finished with value: 0.8552496997074761 and parameters: {'n_estimators': 3700, 'max_depth': 5, 'learning_rate': 0.1069128784702158, 'subsample': 0.6431456404336275, 'colsample_bytree': 0.9140413473084155, 'reg_alpha': 2.822492249830016, 'reg_lambda': 2.102320432811408}. Best is trial 31 with value: 0.8650734897442633.\n",
      "[I 2025-05-14 18:25:19,247] Trial 36 finished with value: 0.8602689357460007 and parameters: {'n_estimators': 3300, 'max_depth': 4, 'learning_rate': 0.06601374857442044, 'subsample': 0.6002881659504267, 'colsample_bytree': 0.9513333923025759, 'reg_alpha': 1.1820967316280127, 'reg_lambda': 1.092427090627497}. Best is trial 31 with value: 0.8650734897442633.\n",
      "[I 2025-05-14 18:25:25,153] Trial 37 finished with value: 0.839813925126548 and parameters: {'n_estimators': 3200, 'max_depth': 3, 'learning_rate': 0.06699457366842128, 'subsample': 0.6261751932915793, 'colsample_bytree': 0.9533400767627044, 'reg_alpha': 1.266620477873945, 'reg_lambda': 0.9758266243528614}. Best is trial 31 with value: 0.8650734897442633.\n",
      "[I 2025-05-14 18:25:34,277] Trial 38 finished with value: 0.8585546359106615 and parameters: {'n_estimators': 2900, 'max_depth': 5, 'learning_rate': 0.09714154158555446, 'subsample': 0.603030588002306, 'colsample_bytree': 0.9781295238822828, 'reg_alpha': 0.009941849252025348, 'reg_lambda': 2.145401076884884}. Best is trial 31 with value: 0.8650734897442633.\n",
      "[I 2025-05-14 18:25:43,493] Trial 39 finished with value: 0.8625090186185999 and parameters: {'n_estimators': 3800, 'max_depth': 4, 'learning_rate': 0.07296161805863889, 'subsample': 0.6550355578309658, 'colsample_bytree': 0.9454860929958422, 'reg_alpha': 0.5185927816257545, 'reg_lambda': 2.5412484024903197}. Best is trial 31 with value: 0.8650734897442633.\n",
      "[I 2025-05-14 18:25:55,441] Trial 40 finished with value: 0.8603175845773028 and parameters: {'n_estimators': 3800, 'max_depth': 5, 'learning_rate': 0.07291501248211141, 'subsample': 0.6633148031700927, 'colsample_bytree': 0.94100533596499, 'reg_alpha': 0.5331243067981996, 'reg_lambda': 2.528290156382709}. Best is trial 31 with value: 0.8650734897442633.\n",
      "[I 2025-05-14 18:26:07,420] Trial 41 finished with value: 0.8653100976469947 and parameters: {'n_estimators': 3800, 'max_depth': 5, 'learning_rate': 0.06853211653659348, 'subsample': 0.6229831607164342, 'colsample_bytree': 0.9454975847893885, 'reg_alpha': 0.5437872538732247, 'reg_lambda': 2.3203603844760643}. Best is trial 41 with value: 0.8653100976469947.\n",
      "[I 2025-05-14 18:26:19,394] Trial 42 finished with value: 0.8630189970836204 and parameters: {'n_estimators': 3800, 'max_depth': 5, 'learning_rate': 0.07103416072731976, 'subsample': 0.6615583588647651, 'colsample_bytree': 0.9379414737691695, 'reg_alpha': 0.3613145891549398, 'reg_lambda': 2.5422498538454916}. Best is trial 41 with value: 0.8653100976469947.\n",
      "[I 2025-05-14 18:26:31,325] Trial 43 finished with value: 0.8631219954922297 and parameters: {'n_estimators': 3800, 'max_depth': 5, 'learning_rate': 0.07751710300596337, 'subsample': 0.6683860707122069, 'colsample_bytree': 0.9366495080616531, 'reg_alpha': 0.47362025420350023, 'reg_lambda': 2.6295414967292334}. Best is trial 41 with value: 0.8653100976469947.\n",
      "[I 2025-05-14 18:26:43,918] Trial 44 finished with value: 0.867095212464889 and parameters: {'n_estimators': 4000, 'max_depth': 5, 'learning_rate': 0.060952378956139926, 'subsample': 0.6652748456442558, 'colsample_bytree': 0.9386443202699265, 'reg_alpha': 0.6156491815858112, 'reg_lambda': 2.659213356126134}. Best is trial 44 with value: 0.867095212464889.\n",
      "[I 2025-05-14 18:26:56,673] Trial 45 finished with value: 0.8679253012119272 and parameters: {'n_estimators': 4000, 'max_depth': 5, 'learning_rate': 0.055025307082608235, 'subsample': 0.6272611512543017, 'colsample_bytree': 0.9867043517730124, 'reg_alpha': 0.5543753187724325, 'reg_lambda': 4.41395878314679}. Best is trial 45 with value: 0.8679253012119272.\n",
      "[I 2025-05-14 18:27:12,265] Trial 46 finished with value: 0.8637653757415238 and parameters: {'n_estimators': 4000, 'max_depth': 6, 'learning_rate': 0.05696210167972044, 'subsample': 0.6267090204242802, 'colsample_bytree': 0.9898999466704247, 'reg_alpha': 1.8952972721113315, 'reg_lambda': 4.005600727627947}. Best is trial 45 with value: 0.8679253012119272.\n",
      "[I 2025-05-14 18:27:27,767] Trial 47 finished with value: 0.8628469993186598 and parameters: {'n_estimators': 4000, 'max_depth': 6, 'learning_rate': 0.05736705725395386, 'subsample': 0.626113949921222, 'colsample_bytree': 0.9964258857337454, 'reg_alpha': 1.707433895427895, 'reg_lambda': 4.530523085835819}. Best is trial 45 with value: 0.8679253012119272.\n",
      "[I 2025-05-14 18:27:46,300] Trial 48 finished with value: 0.8584929821489878 and parameters: {'n_estimators': 4000, 'max_depth': 7, 'learning_rate': 0.04238422421406741, 'subsample': 0.6335337591950376, 'colsample_bytree': 0.9786310322869067, 'reg_alpha': 2.552399626219644, 'reg_lambda': 3.7561919245179514}. Best is trial 45 with value: 0.8679253012119272.\n",
      "[I 2025-05-14 18:28:03,759] Trial 49 finished with value: 0.8544789312105647 and parameters: {'n_estimators': 3600, 'max_depth': 8, 'learning_rate': 0.038143752833131754, 'subsample': 0.6171414370961954, 'colsample_bytree': 0.6071125802074584, 'reg_alpha': 3.4234676888785716, 'reg_lambda': 4.172741368120325}. Best is trial 45 with value: 0.8679253012119272.\n",
      "\n",
      "🎯 Best R² Score: 0.8679253012119272\n",
      "🔥 Best Parameters:\n",
      " {'n_estimators': 4000, 'max_depth': 5, 'learning_rate': 0.055025307082608235, 'subsample': 0.6272611512543017, 'colsample_bytree': 0.9867043517730124, 'reg_alpha': 0.5543753187724325, 'reg_lambda': 4.41395878314679}\n"
     ]
    }
   ],
   "source": [
    "# Optuna run for xgboost\n",
    "X_train_ori = pipeline.fit_transform(train_df)\n",
    "y_train_ori = train_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_ori, y_train_ori, test_size = 0.2, random_state=42)\n",
    "X_train, X_test, cat_features = preprocess_for_model('XGBoost', X_train, X_test)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 4000, step=100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 10.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 10.0),\n",
    "        \"random_state\": 42,\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\", \n",
    "        \"enable_categorical\":True\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return r2_score(y_test, preds)\n",
    "\n",
    "# Run the Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Print best results\n",
    "print(\"\\n🎯 Best R² Score:\", study.best_value)\n",
    "print(\"🔥 Best Parameters:\\n\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caa73adb-9ad3-4206-9396-ea39f3544a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 18:29:56,330] A new study created in memory with name: no-name-cfcef2cc-7631-4263-bb29-6ddbc688c46a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42eb79c6ea854ef2b7d46add0f3369f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 18:32:24,219] Trial 0 finished with value: 0.8216588838200614 and parameters: {'boosting_type': 'Ordered', 'iterations': 3800, 'learning_rate': 0.02430451089389866, 'depth': 9, 'l2_leaf_reg': 4.374671539978969, 'random_strength': 1.24363411539355, 'bagging_temperature': 0.07178134164855798, 'border_count': 220}. Best is trial 0 with value: 0.8216588838200614.\n",
      "[I 2025-05-14 18:32:26,907] Trial 1 finished with value: 0.3897245146133438 and parameters: {'boosting_type': 'Ordered', 'iterations': 400, 'learning_rate': 0.01144089124374292, 'depth': 6, 'l2_leaf_reg': 7.6497446501287305, 'random_strength': 3.426106904956809, 'bagging_temperature': 0.6169544587664344, 'border_count': 163}. Best is trial 0 with value: 0.8216588838200614.\n",
      "[I 2025-05-14 18:33:09,964] Trial 2 finished with value: 0.8276343770288965 and parameters: {'boosting_type': 'Ordered', 'iterations': 1500, 'learning_rate': 0.14915462815960986, 'depth': 9, 'l2_leaf_reg': 9.67023270059699, 'random_strength': 2.5955050281381546, 'bagging_temperature': 0.4182372857163915, 'border_count': 144}. Best is trial 2 with value: 0.8276343770288965.\n",
      "[I 2025-05-14 18:33:25,280] Trial 3 finished with value: 0.8248166541637698 and parameters: {'boosting_type': 'Ordered', 'iterations': 2100, 'learning_rate': 0.16782733837905592, 'depth': 6, 'l2_leaf_reg': 5.477724137229211, 'random_strength': 1.2025098579296045, 'bagging_temperature': 0.6433322446814539, 'border_count': 69}. Best is trial 2 with value: 0.8276343770288965.\n",
      "[I 2025-05-14 18:33:39,228] Trial 4 finished with value: 0.7606649341300604 and parameters: {'boosting_type': 'Ordered', 'iterations': 2500, 'learning_rate': 0.016929713550481323, 'depth': 6, 'l2_leaf_reg': 1.6948756301733994, 'random_strength': 3.3219640310230725, 'bagging_temperature': 0.21984965158838532, 'border_count': 41}. Best is trial 2 with value: 0.8276343770288965.\n",
      "[I 2025-05-14 18:33:47,914] Trial 5 finished with value: 0.8264183531049306 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3700, 'learning_rate': 0.05063982641756142, 'depth': 5, 'l2_leaf_reg': 9.086017653621257, 'random_strength': 2.51021775893929, 'bagging_temperature': 0.189395531759028, 'border_count': 122}. Best is trial 2 with value: 0.8276343770288965.\n",
      "[I 2025-05-14 18:33:48,767] Trial 6 finished with value: 0.7501313098121235 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'Depthwise', 'iterations': 300, 'learning_rate': 0.09592522015528816, 'depth': 5, 'l2_leaf_reg': 5.290277477479819, 'random_strength': 3.869510770199746, 'bagging_temperature': 0.8764777771833787, 'border_count': 79}. Best is trial 2 with value: 0.8276343770288965.\n",
      "[I 2025-05-14 18:33:51,040] Trial 7 finished with value: 0.7460307930004488 and parameters: {'boosting_type': 'Ordered', 'iterations': 300, 'learning_rate': 0.17922499711688064, 'depth': 5, 'l2_leaf_reg': 3.32852199059464, 'random_strength': 4.180474833275523, 'bagging_temperature': 0.05308604058351518, 'border_count': 205}. Best is trial 2 with value: 0.8276343770288965.\n",
      "[I 2025-05-14 18:33:56,894] Trial 8 finished with value: 0.7923272864722575 and parameters: {'boosting_type': 'Ordered', 'iterations': 4000, 'learning_rate': 0.2588527280660814, 'depth': 4, 'l2_leaf_reg': 2.102994164573002, 'random_strength': 4.74099263953581, 'bagging_temperature': 0.19468509835132952, 'border_count': 172}. Best is trial 2 with value: 0.8276343770288965.\n",
      "[I 2025-05-14 18:34:56,560] Trial 9 finished with value: 0.816082223667939 and parameters: {'boosting_type': 'Ordered', 'iterations': 3600, 'learning_rate': 0.25841628678144984, 'depth': 9, 'l2_leaf_reg': 9.692380300609365, 'random_strength': 4.273064932955106, 'bagging_temperature': 0.15249629728152214, 'border_count': 165}. Best is trial 2 with value: 0.8276343770288965.\n",
      "[I 2025-05-14 18:35:06,123] Trial 10 finished with value: 0.8261228612113829 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'Lossguide', 'iterations': 1500, 'learning_rate': 0.0650813260046699, 'depth': 10, 'l2_leaf_reg': 7.751241068340939, 'random_strength': 0.16750666759506583, 'bagging_temperature': 0.40483716798453656, 'border_count': 113}. Best is trial 2 with value: 0.8276343770288965.\n",
      "[I 2025-05-14 18:35:24,600] Trial 11 finished with value: 0.8433285422392316 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 2900, 'learning_rate': 0.03495086494411736, 'depth': 8, 'l2_leaf_reg': 9.637822648501826, 'random_strength': 2.325279432980941, 'bagging_temperature': 0.362925188400985, 'border_count': 112}. Best is trial 11 with value: 0.8433285422392316.\n",
      "[I 2025-05-14 18:35:42,643] Trial 12 finished with value: 0.844896678649147 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 2800, 'learning_rate': 0.03797555676747754, 'depth': 8, 'l2_leaf_reg': 7.942853089040931, 'random_strength': 2.221593394694769, 'bagging_temperature': 0.4079411506702617, 'border_count': 114}. Best is trial 12 with value: 0.844896678649147.\n",
      "[I 2025-05-14 18:36:00,611] Trial 13 finished with value: 0.8446992980931493 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 2900, 'learning_rate': 0.034522764614486126, 'depth': 8, 'l2_leaf_reg': 6.952803823924802, 'random_strength': 1.6498025738438573, 'bagging_temperature': 0.34842097249589055, 'border_count': 97}. Best is trial 12 with value: 0.844896678649147.\n",
      "[I 2025-05-14 18:36:17,451] Trial 14 finished with value: 0.8458424797778568 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3100, 'learning_rate': 0.03151843368183365, 'depth': 8, 'l2_leaf_reg': 7.108710925826511, 'random_strength': 1.5941536361984676, 'bagging_temperature': 0.5884246111080519, 'border_count': 81}. Best is trial 14 with value: 0.8458424797778568.\n",
      "[I 2025-05-14 18:36:27,489] Trial 15 finished with value: 0.818678533890763 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 2800, 'learning_rate': 0.021981251654587756, 'depth': 7, 'l2_leaf_reg': 6.767367683607264, 'random_strength': 0.15061198350827354, 'bagging_temperature': 0.6021670504925691, 'border_count': 33}. Best is trial 14 with value: 0.8458424797778568.\n",
      "[I 2025-05-14 18:36:46,342] Trial 16 finished with value: 0.8520862256342513 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3200, 'learning_rate': 0.060615545667855035, 'depth': 8, 'l2_leaf_reg': 6.516451204184648, 'random_strength': 1.8496247286813223, 'bagging_temperature': 0.8354764517542416, 'border_count': 67}. Best is trial 16 with value: 0.8520862256342513.\n",
      "[I 2025-05-14 18:37:03,527] Trial 17 finished with value: 0.831089392958462 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'Lossguide', 'iterations': 3400, 'learning_rate': 0.09239721692575409, 'depth': 7, 'l2_leaf_reg': 6.335132438518956, 'random_strength': 0.8421096140440634, 'bagging_temperature': 0.9071605877397446, 'border_count': 63}. Best is trial 16 with value: 0.8520862256342513.\n",
      "[I 2025-05-14 18:37:59,868] Trial 18 finished with value: 0.8098876261089255 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'Depthwise', 'iterations': 3300, 'learning_rate': 0.06717965145827005, 'depth': 10, 'l2_leaf_reg': 4.381911863599392, 'random_strength': 1.9702492293860627, 'bagging_temperature': 0.792995993336503, 'border_count': 54}. Best is trial 16 with value: 0.8520862256342513.\n",
      "[I 2025-05-14 18:38:11,701] Trial 19 finished with value: 0.7538671923135857 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 2100, 'learning_rate': 0.010994014144215483, 'depth': 8, 'l2_leaf_reg': 6.236869669514432, 'random_strength': 2.9993881773588495, 'bagging_temperature': 0.7369020640968043, 'border_count': 85}. Best is trial 16 with value: 0.8520862256342513.\n",
      "[I 2025-05-14 18:38:27,166] Trial 20 finished with value: 0.8462885379570226 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3200, 'learning_rate': 0.045847762059642895, 'depth': 7, 'l2_leaf_reg': 8.630346897173693, 'random_strength': 0.6654981520157723, 'bagging_temperature': 0.9521114469143287, 'border_count': 143}. Best is trial 16 with value: 0.8520862256342513.\n",
      "[I 2025-05-14 18:38:41,539] Trial 21 finished with value: 0.8482646910000172 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3200, 'learning_rate': 0.05030059330149416, 'depth': 7, 'l2_leaf_reg': 8.511884328116663, 'random_strength': 0.6847314429626754, 'bagging_temperature': 0.9946995538312166, 'border_count': 190}. Best is trial 16 with value: 0.8520862256342513.\n",
      "[I 2025-05-14 18:38:53,168] Trial 22 finished with value: 0.8407423222127299 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 2400, 'learning_rate': 0.04756910091227042, 'depth': 7, 'l2_leaf_reg': 8.430922744506537, 'random_strength': 0.6913164600794283, 'bagging_temperature': 0.991570176511636, 'border_count': 244}. Best is trial 16 with value: 0.8520862256342513.\n",
      "[I 2025-05-14 18:39:08,537] Trial 23 finished with value: 0.8524985016509024 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3400, 'learning_rate': 0.09760652345659593, 'depth': 7, 'l2_leaf_reg': 8.58582675594469, 'random_strength': 0.7416112244130186, 'bagging_temperature': 0.9946352944324367, 'border_count': 193}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:39:27,279] Trial 24 finished with value: 0.8510225369236849 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 4000, 'learning_rate': 0.10108730115636747, 'depth': 7, 'l2_leaf_reg': 8.827148424891526, 'random_strength': 1.1668339485518664, 'bagging_temperature': 0.8150116494969595, 'border_count': 203}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:39:48,872] Trial 25 finished with value: 0.8315687084643524 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'Lossguide', 'iterations': 4000, 'learning_rate': 0.10249286277578282, 'depth': 6, 'l2_leaf_reg': 9.005187030455863, 'random_strength': 1.3215674580766295, 'bagging_temperature': 0.8194403504142087, 'border_count': 248}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:40:31,334] Trial 26 finished with value: 0.8099156478590446 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'Depthwise', 'iterations': 3500, 'learning_rate': 0.12899193203243925, 'depth': 9, 'l2_leaf_reg': 7.591684700310723, 'random_strength': 1.8154741362796956, 'bagging_temperature': 0.7154233727303195, 'border_count': 225}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:40:56,457] Trial 27 finished with value: 0.8510356986539613 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3800, 'learning_rate': 0.07369650600831489, 'depth': 8, 'l2_leaf_reg': 4.548106988459469, 'random_strength': 1.0017449845636248, 'bagging_temperature': 0.8568493850576847, 'border_count': 189}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:41:07,039] Trial 28 finished with value: 0.848329755502367 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 1600, 'learning_rate': 0.07204054646808121, 'depth': 8, 'l2_leaf_reg': 4.247585427050332, 'random_strength': 0.0006968879628781455, 'bagging_temperature': 0.885774911785623, 'border_count': 188}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:41:41,589] Trial 29 finished with value: 0.8520158223964298 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3700, 'learning_rate': 0.0773656135173758, 'depth': 9, 'l2_leaf_reg': 4.581213186740147, 'random_strength': 0.9720308731889863, 'bagging_temperature': 0.5055639668703982, 'border_count': 225}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:42:22,677] Trial 30 finished with value: 0.8064437795234757 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'Depthwise', 'iterations': 3500, 'learning_rate': 0.1225924780550195, 'depth': 10, 'l2_leaf_reg': 3.0337560801962646, 'random_strength': 0.4030584571410937, 'bagging_temperature': 0.7379397139491597, 'border_count': 222}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:42:59,006] Trial 31 finished with value: 0.8499766233937036 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3800, 'learning_rate': 0.07887469023981372, 'depth': 9, 'l2_leaf_reg': 4.728053265163673, 'random_strength': 1.0124308876491506, 'bagging_temperature': 0.52225260176928, 'border_count': 233}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:43:33,095] Trial 32 finished with value: 0.8443549618381823 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3700, 'learning_rate': 0.08191272908087381, 'depth': 9, 'l2_leaf_reg': 3.479336664916831, 'random_strength': 1.46994209909216, 'bagging_temperature': 0.915482046985658, 'border_count': 206}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:43:40,561] Trial 33 finished with value: 0.8323672420909071 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 1000, 'learning_rate': 0.05936321437828991, 'depth': 8, 'l2_leaf_reg': 5.976874365023125, 'random_strength': 0.42838349222167427, 'bagging_temperature': 0.6872278913946409, 'border_count': 145}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:44:02,012] Trial 34 finished with value: 0.8453449150506859 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 2600, 'learning_rate': 0.12353336721989903, 'depth': 9, 'l2_leaf_reg': 5.014289494718213, 'random_strength': 0.9799850086722313, 'bagging_temperature': 0.4837639146617294, 'border_count': 188}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:44:41,380] Trial 35 finished with value: 0.8247984294770827 and parameters: {'boosting_type': 'Ordered', 'iterations': 3800, 'learning_rate': 0.17852686271754348, 'depth': 8, 'l2_leaf_reg': 3.919172465313247, 'random_strength': 1.3383504689984758, 'bagging_temperature': 0.8361127431620484, 'border_count': 175}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:44:54,361] Trial 36 finished with value: 0.8253106353571951 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'Lossguide', 'iterations': 3100, 'learning_rate': 0.14733466667121173, 'depth': 6, 'l2_leaf_reg': 5.750142935890991, 'random_strength': 1.9994727491978828, 'bagging_temperature': 0.7719152856146111, 'border_count': 153}. Best is trial 23 with value: 0.8524985016509024.\n",
      "[I 2025-05-14 18:45:25,998] Trial 37 finished with value: 0.8540144809847059 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3400, 'learning_rate': 0.05700969868853112, 'depth': 9, 'l2_leaf_reg': 2.7443753572688863, 'random_strength': 0.4426992953379003, 'bagging_temperature': 0.9537577383590397, 'border_count': 207}. Best is trial 37 with value: 0.8540144809847059.\n",
      "[I 2025-05-14 18:48:33,397] Trial 38 finished with value: 0.811389927647415 and parameters: {'boosting_type': 'Ordered', 'iterations': 2300, 'learning_rate': 0.022652231547338, 'depth': 10, 'l2_leaf_reg': 2.447792509467352, 'random_strength': 0.4267287914876941, 'bagging_temperature': 0.9353822151384015, 'border_count': 254}. Best is trial 37 with value: 0.8540144809847059.\n",
      "[I 2025-05-14 18:49:06,063] Trial 39 finished with value: 0.8408600689014107 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3400, 'learning_rate': 0.04229462823784208, 'depth': 9, 'l2_leaf_reg': 1.2306968188935583, 'random_strength': 2.853573879229253, 'bagging_temperature': 0.2699415695811958, 'border_count': 212}. Best is trial 37 with value: 0.8540144809847059.\n",
      "[I 2025-05-14 18:51:26,216] Trial 40 finished with value: 0.8258050784651019 and parameters: {'boosting_type': 'Ordered', 'iterations': 1800, 'learning_rate': 0.05972196621964913, 'depth': 10, 'l2_leaf_reg': 2.6940737914776833, 'random_strength': 0.36977007067562084, 'bagging_temperature': 0.6706680663152025, 'border_count': 237}. Best is trial 37 with value: 0.8540144809847059.\n",
      "[I 2025-05-14 18:52:01,692] Trial 41 finished with value: 0.8443692209260119 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3800, 'learning_rate': 0.08344119839815273, 'depth': 9, 'l2_leaf_reg': 3.722924037186553, 'random_strength': 1.1416376058267925, 'bagging_temperature': 0.8475354727664072, 'border_count': 193}. Best is trial 37 with value: 0.8540144809847059.\n",
      "[I 2025-05-14 18:52:36,684] Trial 42 finished with value: 0.855513430862866 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3600, 'learning_rate': 0.054179676266114606, 'depth': 9, 'l2_leaf_reg': 5.425455174508331, 'random_strength': 0.632030143949728, 'bagging_temperature': 0.9513670435640664, 'border_count': 215}. Best is trial 42 with value: 0.855513430862866.\n",
      "[I 2025-05-14 18:53:11,470] Trial 43 finished with value: 0.8521629514531905 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3600, 'learning_rate': 0.029700685737159557, 'depth': 9, 'l2_leaf_reg': 5.3319382101594215, 'random_strength': 0.7800042444630506, 'bagging_temperature': 0.9608238967358258, 'border_count': 214}. Best is trial 42 with value: 0.855513430862866.\n",
      "[I 2025-05-14 18:53:54,426] Trial 44 finished with value: 0.8492574897424099 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3000, 'learning_rate': 0.028977387223926284, 'depth': 10, 'l2_leaf_reg': 5.33268578149451, 'random_strength': 0.7763877853355978, 'bagging_temperature': 0.9936520580186001, 'border_count': 213}. Best is trial 42 with value: 0.855513430862866.\n",
      "[I 2025-05-14 18:54:17,107] Trial 45 finished with value: 0.8306933783914661 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 2600, 'learning_rate': 0.016985036731672346, 'depth': 9, 'l2_leaf_reg': 5.645055758472426, 'random_strength': 0.22177382148989055, 'bagging_temperature': 0.9497808071612576, 'border_count': 178}. Best is trial 42 with value: 0.855513430862866.\n",
      "[I 2025-05-14 18:55:24,004] Trial 46 finished with value: 0.8277730067715758 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'Depthwise', 'iterations': 3500, 'learning_rate': 0.02636623189028691, 'depth': 9, 'l2_leaf_reg': 7.370292233552936, 'random_strength': 3.4950017994352325, 'bagging_temperature': 0.887089157798595, 'border_count': 161}. Best is trial 42 with value: 0.855513430862866.\n",
      "[I 2025-05-14 18:55:26,614] Trial 47 finished with value: 0.5842762653037235 and parameters: {'boosting_type': 'Ordered', 'iterations': 800, 'learning_rate': 0.014924498688650482, 'depth': 4, 'l2_leaf_reg': 9.980201408968275, 'random_strength': 0.5593260551436336, 'bagging_temperature': 0.9551085419289219, 'border_count': 129}. Best is trial 42 with value: 0.855513430862866.\n",
      "[I 2025-05-14 18:55:49,946] Trial 48 finished with value: 0.8503345816393566 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3300, 'learning_rate': 0.05551477599827676, 'depth': 8, 'l2_leaf_reg': 6.517577582421914, 'random_strength': 1.5262031575161341, 'bagging_temperature': 0.9083424124956927, 'border_count': 197}. Best is trial 42 with value: 0.855513430862866.\n",
      "[I 2025-05-14 18:56:11,769] Trial 49 finished with value: 0.8349759351690045 and parameters: {'boosting_type': 'Plain', 'grow_policy': 'Lossguide', 'iterations': 3600, 'learning_rate': 0.03969648239527748, 'depth': 7, 'l2_leaf_reg': 1.8702236308803402, 'random_strength': 4.8743347140366655, 'bagging_temperature': 0.7735007383962706, 'border_count': 214}. Best is trial 42 with value: 0.855513430862866.\n",
      "\n",
      "🔥 Best R² Score: 0.855513430862866\n",
      "📦 Best Parameters:\n",
      " {'boosting_type': 'Plain', 'grow_policy': 'SymmetricTree', 'iterations': 3600, 'learning_rate': 0.054179676266114606, 'depth': 9, 'l2_leaf_reg': 5.425455174508331, 'random_strength': 0.632030143949728, 'bagging_temperature': 0.9513670435640664, 'border_count': 215}\n"
     ]
    }
   ],
   "source": [
    "# Optuna run for cat boost\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_ori, y_train_ori, test_size = 0.2, random_state=42)\n",
    "X_train_mod, X_test_mod, cat_features = preprocess_for_model('CatBoost', X_train, X_test)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    boosting_type = trial.suggest_categorical(\"boosting_type\", [\"Plain\", \"Ordered\"])\n",
    "    if boosting_type == \"Ordered\":\n",
    "        grow_policy = \"SymmetricTree\"\n",
    "    else:\n",
    "        grow_policy = trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"])\n",
    "    \n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 300, 4000, step=100),\n",
    "        \"early_stopping_rounds\": 50,\n",
    "        \"verbose\": 0,\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"eval_metric\": \"R2\",\n",
    "        \"boosting_type\": boosting_type,\n",
    "        \"grow_policy\": grow_policy,\n",
    "\n",
    "        # Hyperparameters to tune\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.0, 5.0),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Plain\", \"Ordered\"]),\n",
    "    }\n",
    "\n",
    "    # CatBoost pool for better handling of validation set\n",
    "    cat_features = [col for col in X_train.columns if X_train[col].dtype.name == 'category']\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "    valid_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
    "    \n",
    "    # train_pool = Pool(X_train, y_train)\n",
    "    # valid_pool = Pool(X_test, y_test)\n",
    "\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(train_pool, eval_set=valid_pool)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    return r2_score(y_test, preds)\n",
    "\n",
    "# Run the Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Print best results\n",
    "print(\"\\n🔥 Best R² Score:\", study.best_value)\n",
    "print(\"📦 Best Parameters:\\n\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8146894-5044-4ef7-a80f-57f0bb2a6c9a",
   "metadata": {},
   "source": [
    "# Old code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a6f8b4-3462-4422-8ff9-45c50d0a8e63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# PolynomialFeatures prediction test\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_train_ori \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mfit_transform(train_df)\n\u001b[1;32m      4\u001b[0m X_test_ori \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mfit_transform(test_df)\n\u001b[1;32m      5\u001b[0m y_train_ori \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# PolynomialFeatures prediction test\n",
    "\n",
    "X_train_ori = pipeline.fit_transform(train_df)\n",
    "X_test_ori = pipeline.fit_transform(test_df)\n",
    "y_train_ori = train_df['target']\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "X_poly_process = X_train_ori.copy()\n",
    "X_test_process = X_test_ori.copy()\n",
    "cat_cols = X_poly_process.select_dtypes(['category']).columns\n",
    "X_poly_process[cat_cols] = X_poly_process[cat_cols].astype('int')\n",
    "X_test_process[cat_cols] = X_test_process[cat_cols].astype('int')\n",
    "X_poly_process = X_poly_process.fillna(0)\n",
    "X_test_process = X_test_process.fillna(0)\n",
    "\n",
    "X_poly = poly.fit_transform(X_poly_process)\n",
    "X_poly_test = poly.fit_transform(X_test_process)\n",
    "\n",
    "feature_names = poly.get_feature_names_out(X_poly_process.columns)\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=feature_names)\n",
    "X_poly_test_df = pd.DataFrame(X_poly_test, columns=feature_names)\n",
    "# X_poly_df.drop(columns=X_train_ori.columns, inplace=True)\n",
    "\n",
    "\n",
    "# Step 1: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly_df, y_train_ori, test_size=0.2, random_state=42\n",
    ")\n",
    "print(X_train.shape)\n",
    "print(f\"X_poly_test_df shape: {X_poly_test_df.shape}\")\n",
    "\n",
    "models = {\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42\n",
    "                                  # , depth=8, iterations=2500\n",
    "                                 )\n",
    "    # ,\"XGBoost\": XGBRegressor(\n",
    "    #     n_estimators=100,\n",
    "    #     random_state=42,\n",
    "    #     tree_method='hist',\n",
    "    #     enable_categorical=True\n",
    "    # ),\n",
    "\n",
    "    # \"LightGBM\": LGBMRegressor(\n",
    "    #     n_estimators=100,\n",
    "    #     random_state=42,\n",
    "    #     verbose=-1\n",
    "    # ),\n",
    "\n",
    "    # \"RandomForest\": RandomForestRegressor(\n",
    "    #     n_estimators=100,\n",
    "    #     max_depth=None,\n",
    "    #     min_samples_split=2,\n",
    "    #     min_samples_leaf=1,\n",
    "    #     n_jobs=-1,\n",
    "    #     random_state=42,\n",
    "    #     verbose=0\n",
    "    # ),\n",
    "\n",
    "    # \"Ridge\": Ridge(alpha=1.0),\n",
    "\n",
    "    # \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000, random_state=42)\n",
    "\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name} R² score: {r2:.4f}\")\n",
    "\n",
    "    if name == \"CatBoost\":\n",
    "        saved_catboost_model = model\n",
    "\n",
    "    # Submit logic\n",
    "    # subs = model.predict(X_poly_test_df)\n",
    "    # submit_df = pd.DataFrame({'id': range(len(subs)), 'target':subs})\n",
    "    # submit_df.to_csv('submit.csv', index = False)\n",
    "    \n",
    "\n",
    "    # Check for feature importance support\n",
    "    # if hasattr(model, \"feature_importances_\"):\n",
    "    #     importances = np.array(model.feature_importances_)\n",
    "    #     features = X_train.columns\n",
    "\n",
    "    #     # Sort top 30\n",
    "    #     sorted_indices = importances.argsort()[::-1]\n",
    "    #     print(f\"\\nTop 30 features for {name}:\")\n",
    "    #     for i in sorted_indices:\n",
    "    #         print(f\"{features[i]}: {importances[i]:.4f}\")\n",
    "    # else:\n",
    "    #     print(f\"{name} does not support feature_importances_\")\n",
    "\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f3bb2-cb75-4898-a59a-98db2285f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌀 CPU Warm-up (Heavy) starting...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"🌀 CPU Warm-up (Heavy) starting...\")\n",
    "start = time.time()\n",
    "\n",
    "# Matrix multiplication on large matrices for a longer duration\n",
    "for _ in range(200):  # Increase iterations for longer warm-up\n",
    "    a = np.random.rand(4000, 4000)\n",
    "    b = np.random.rand(4000, 4000)\n",
    "    c = np.dot(a, b)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"✅ Warm-up complete in {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3609e487-985c-4d1f-b6d5-a21e3edfbd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost R² with top 3000 features: 0.8298\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get top 3000 features\n",
    "if hasattr(saved_catboost_model, \"feature_importances_\"):\n",
    "    importances = np.array(saved_catboost_model.feature_importances_)\n",
    "    features = X_poly_df.columns\n",
    "    sorted_indices = importances.argsort()[::-1][:3000]\n",
    "    top_3000_features = [features[i] for i in sorted_indices]\n",
    "else:\n",
    "    raise ValueError(\"Model does not support feature_importances_\")\n",
    "\n",
    "# Step 2: Slice the DataFrame\n",
    "X_train_top3000 = X_train[top_3000_features]\n",
    "\n",
    "    # Step 3: Train CatBoost with reduced features\n",
    "model_top3000 = CatBoostRegressor(verbose=0, random_state=42)\n",
    "model_top3000.fit(X_train_top3000, y_train)\n",
    "\n",
    "# Step 4: Evaluate\n",
    "preds = model_top3000.predict(X_test[top_3000_features])\n",
    "r2 = r2_score(y_test, preds)\n",
    "print(f\"CatBoost R² with top 3000 features: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9694373-715c-48eb-91be-3d2a5ae7ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost R² score: 0.8274\n",
    "\n",
    "Top 30 features for CatBoost:\n",
    "vapor_pressure_n_late_evening_mean week_cos: 5.9547\n",
    "dew_point_n_late_evening_mean humidity_n_late_evening_mean: 2.0209\n",
    "dew_point_n_late_evening_mean humidity_n_afternoon_mean: 1.5551\n",
    "vapor_pressure_n_evening_mean week_cos: 1.4185\n",
    "vapor_pressure_n_late_evening_mean month_cos: 1.2828\n",
    "dew_point_n_late_evening_mean humidity_n_evening_mean: 1.2668\n",
    "humidity_n_late_evening_mean climatology_temp: 0.9876\n",
    "surface_temp_n_late_evening_mean week_sin: 0.8565\n",
    "visibility_n_morning_mean climatology_temp: 0.7640\n",
    "humidity_n_evening_mean humidity_n_range: 0.7342\n",
    "vapor_pressure_n_evening_mean climatology_temp: 0.6970\n",
    "dew_point_n_late_evening_mean week_cos: 0.6267\n",
    "climatology_temp month: 0.5823\n",
    "surface_temp_n_when_max vapor_pressure_n_late_evening_mean: 0.5606\n",
    "surface_temp_n_late_evening_mean month_sin: 0.5475\n",
    "humidity_n_evening_mean climatology_temp: 0.5304\n",
    "wind_speed_n_range climatology_temp: 0.5263\n",
    "vapor_pressure_n_late_evening_mean week_sin: 0.5110\n",
    "climatology_temp week_of_year: 0.4830\n",
    "local_pressure_n_afternoon_mean vapor_pressure_n_late_evening_mean: 0.4803\n",
    "snow_depth_n_when_max climatology_temp: 0.4695\n",
    "vapor_pressure_n_afternoon_mean climatology_temp: 0.4689\n",
    "humidity_n_evening_mean wind_speed_n_range: 0.4253\n",
    "vapor_pressure_n_late_evening_mean climatology_temp: 0.4207\n",
    "dew_point_n_evening_mean climatology_temp: 0.4083\n",
    "humidity_n_late_evening_mean week_sin: 0.4040\n",
    "dew_point_n_late_evening_mean week_of_year: 0.4038\n",
    "dew_point_n_late_evening_mean visibility_n_early_night_mean: 0.3928\n",
    "surface_temp_n_late_evening_mean quarter_sin: 0.3727\n",
    "surface_temp_n_evening_mean climatology_temp: 0.3692\n",
    "surface_temp_n_afternoon_mean climatology_temp: 0.3680\n",
    "dew_point_n_morning_mean climatology_temp: 0.3663\n",
    "sea_level_pressure_n_afternoon_mean climatology_temp: 0.3641\n",
    "sea_level_pressure_n_late_evening_mean surface_temp_n_evening_mean: 0.3625\n",
    "dew_point_n_late_evening_mean humidity_n_when_min: 0.3618\n",
    "surface_temp_n_evening_mean quarter_sin: 0.3562\n",
    "cloud_cover_n_afternoon_mean dew_point_n_late_evening_mean: 0.3540\n",
    "dew_point_n_late_evening_mean climatology_temp: 0.3519\n",
    "vapor_pressure_n_late_night_mean week_sin: 0.3507\n",
    "surface_temp_n_evening_mean week_cos: 0.3473\n",
    "\n",
    "XGBoost R² score: 0.9791\n",
    "\n",
    "Top 30 features for XGBoost:\n",
    "dew_point_n_late_evening_mean humidity_n_afternoon_mean: 0.0821\n",
    "wind_speed_n_range week_of_year: 0.0219\n",
    "vapor_pressure_n_late_evening_mean week_cos: 0.0203\n",
    "snow_depth_n_when_max week_sin: 0.0185\n",
    "surface_temp_n_evening_mean month_sin: 0.0149\n",
    "surface_temp_n_afternoon_mean quarter_sin: 0.0132\n",
    "surface_temp_n_late_evening_mean quarter: 0.0121\n",
    "dew_point_n_late_evening_mean quarter: 0.0119\n",
    "climatology_temp fog_factor: 0.0111\n",
    "dew_point_n_late_evening_mean humidity_n_late_evening_mean: 0.0108\n",
    "precipitation_n_when_max vapor_pressure_n_late_evening_mean: 0.0105\n",
    "precipitation_n_total_sum day_of_year: 0.0101\n",
    "humidity_n_morning_mean week_sin: 0.0098\n",
    "precipitation_n_range week_cos: 0.0095\n",
    "precipitation_n_when_max vapor_pressure_n_evening_mean: 0.0094\n",
    "sea_level_pressure_n_late_night_mean day_of_year: 0.0077\n",
    "surface_temp_n_when_min wind_speed_n_evening_mean: 0.0075\n",
    "dew_point_n_late_evening_mean humidity_n_evening_mean: 0.0072\n",
    "surface_temp_n_total_mean week_sin: 0.0072\n",
    "week_sin week_cos: 0.0065\n",
    "local_pressure_n_range precipitation_n_last_condition_hour: 0.0064\n",
    "visibility_n_range wind_speed_n_range: 0.0063\n",
    "sunshine_duration_n_morning_mean vapor_pressure_n_afternoon_mean: 0.0062\n",
    "sunshine_duration_n_morning_mean surface_temp_n_total_mean: 0.0056\n",
    "surface_temp_n_morning_mean surface_temp_n_late_evening_mean: 0.0055\n",
    "visibility_n_morning_mean wind_speed_n_evening_mean: 0.0054\n",
    "surface_temp_n_late_evening_mean day_of_year: 0.0053\n",
    "month_sin week_sin: 0.0052\n",
    "vapor_pressure_n_early_night_mean climatology_temp: 0.0050\n",
    "dew_point_n_evening_mean sunshine_duration_n_last_condition_hour: 0.0049\n",
    "surface_temp_n_late_evening_mean month_cos: 0.0048\n",
    "surface_temp_n_late_evening_mean week_sin: 0.0047\n",
    "sunshine_duration_n_last_condition_hour surface_temp_n_late_evening_mean: 0.0047\n",
    "vapor_pressure_n_afternoon_mean wind_speed_n_late_night_mean: 0.0046\n",
    "local_pressure_n_when_max visibility_n_evening_mean: 0.0045\n",
    "surface_temp_n_late_evening_mean vapor_pressure_n_afternoon_mean: 0.0044\n",
    "surface_temp_n_when_min fog_factor: 0.0043\n",
    "dew_point_n_late_evening_mean climatology_temp: 0.0040\n",
    "sea_level_pressure_n_range vapor_pressure_n_late_evening_mean: 0.0040\n",
    "dew_point_n_late_night_mean day_of_year: 0.0039\n",
    "\n",
    "LightGBM R² score: 0.9093\n",
    "\n",
    "Top 30 features for LightGBM:\n",
    "humidity_n_late_evening_mean climatology_temp: 31.0000\n",
    "vapor_pressure_n_late_evening_mean week_cos: 26.0000\n",
    "dew_point_n_late_evening_mean humidity_n_late_evening_mean: 17.0000\n",
    "dew_point_n_late_evening_mean month_cos: 15.0000\n",
    "vapor_pressure_n_late_evening_mean week_sin: 14.0000\n",
    "climatology_temp week_of_year: 14.0000\n",
    "surface_temp_n_late_evening_mean week_sin: 13.0000\n",
    "climatology_temp week_cos: 13.0000\n",
    "humidity_n_evening_mean climatology_temp: 12.0000\n",
    "dew_point_n_late_evening_mean week_cos: 11.0000\n",
    "dew_point_n_late_evening_mean week_sin: 11.0000\n",
    "local_pressure_n_when_min climatology_temp: 11.0000\n",
    "dew_point_n_late_evening_mean climatology_temp: 10.0000\n",
    "climatology_temp month: 10.0000\n",
    "surface_temp_n_late_evening_mean week_cos: 10.0000\n",
    "humidity_n_evening_mean humidity_n_range: 10.0000\n",
    "dew_point_n_evening_mean climatology_temp: 10.0000\n",
    "dew_point_n_evening_mean humidity_n_late_evening_mean: 9.0000\n",
    "humidity_n_late_night_mean humidity_n_late_evening_mean: 9.0000\n",
    "surface_temp_n_late_evening_mean month_cos: 9.0000\n",
    "climatology_temp week_sin: 9.0000\n",
    "sunshine_duration_n_evening_mean vapor_pressure_n_late_evening_mean: 8.0000\n",
    "climatology_temp month_sin: 8.0000\n",
    "surface_temp_n_evening_mean week_cos: 8.0000\n",
    "vapor_pressure_n_evening_mean climatology_temp: 8.0000\n",
    "dew_point_n_early_night_mean week_cos: 8.0000\n",
    "month_cos day_sin: 8.0000\n",
    "dew_point_n_late_evening_mean humidity_n_evening_mean: 8.0000\n",
    "climatology_temp month_cos: 8.0000\n",
    "humidity_n_late_night_mean climatology_temp: 7.0000\n",
    "local_pressure_n_when_min day_sin: 7.0000\n",
    "climatology_temp day_of_week: 7.0000\n",
    "day_sin dow_sin: 7.0000\n",
    "vapor_pressure_n_evening_mean week_cos: 7.0000\n",
    "vapor_pressure_n_evening_mean week_sin: 7.0000\n",
    "vapor_pressure_n_late_evening_mean month_cos: 7.0000\n",
    "humidity_n_early_night_mean climatology_temp: 7.0000\n",
    "vapor_pressure_n_late_evening_mean wind_speed_n_late_night_mean: 7.0000\n",
    "vapor_pressure_n_late_evening_mean climatology_temp: 7.0000\n",
    "snow_depth_n_when_max climatology_temp: 7.0000\n",
    "\n",
    "RandomForest R² score: 0.9599\n",
    "\n",
    "Top 30 features for RandomForest:\n",
    "vapor_pressure_n_late_evening_mean week_cos: 0.1071\n",
    "dew_point_n_late_evening_mean humidity_n_afternoon_mean: 0.0399\n",
    "dew_point_n_late_evening_mean humidity_n_evening_mean: 0.0396\n",
    "dew_point_n_late_evening_mean humidity_n_late_evening_mean: 0.0277\n",
    "dew_point_n_late_evening_mean climatology_temp: 0.0147\n",
    "surface_temp_n_late_evening_mean week_sin: 0.0146\n",
    "precipitation_n_when_max vapor_pressure_n_evening_mean: 0.0103\n",
    "surface_temp_n_evening_mean week_cos: 0.0069\n",
    "surface_temp_n_late_evening_mean day_of_year: 0.0058\n",
    "precipitation_n_when_min vapor_pressure_n_evening_mean: 0.0057\n",
    "surface_temp_n_total_mean week_sin: 0.0057\n",
    "surface_temp_n_evening_mean climatology_temp: 0.0054\n",
    "surface_temp_n_when_min wind_speed_n_evening_mean: 0.0050\n",
    "wind_speed_n_range day_of_year: 0.0047\n",
    "surface_temp_n_late_evening_mean month_cos: 0.0046\n",
    "dew_point_n_late_evening_mean week_cos: 0.0044\n",
    "precipitation_n_when_max vapor_pressure_n_late_evening_mean: 0.0041\n",
    "sunshine_duration_n_evening_mean vapor_pressure_n_late_evening_mean: 0.0039\n",
    "surface_temp_n_late_evening_mean week_cos: 0.0038\n",
    "humidity_n_range week_sin: 0.0037\n",
    "dew_point_n_late_evening_mean month_cos: 0.0037\n",
    "climatology_temp week_cos: 0.0035\n",
    "precipitation_n_when_min vapor_pressure_n_late_evening_mean: 0.0035\n",
    "surface_temp_n_evening_mean month_sin: 0.0035\n",
    "snow_depth_n_when_max week_sin: 0.0034\n",
    "surface_temp_n_when_min wind_speed_n_afternoon_mean: 0.0033\n",
    "snow_depth_n_when_max climatology_temp: 0.0032\n",
    "wind_speed_n_range week_of_year: 0.0031\n",
    "climatology_temp day_of_year: 0.0029\n",
    "surface_temp_n_late_evening_mean week_of_year: 0.0029\n",
    "surface_temp_n_evening_mean week_sin: 0.0029\n",
    "wind_speed_n_afternoon_mean week_of_year: 0.0029\n",
    "dew_point_n_evening_mean climatology_temp: 0.0028\n",
    "surface_temp_n_late_evening_mean month_sin: 0.0028\n",
    "humidity_n_morning_mean week_sin: 0.0027\n",
    "wind_speed_n_afternoon_mean day_of_year: 0.0025\n",
    "vapor_pressure_n_evening_mean climatology_temp: 0.0025\n",
    "vapor_pressure_n_late_evening_mean climatology_temp: 0.0024\n",
    "humidity_n_late_evening_mean climatology_temp: 0.0023\n",
    "dew_point_n_late_evening_mean surface_temp_n_range: 0.0022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418579a-cb2a-4f04-b315-cb26ce4df4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit logic\n",
    "\n",
    "# Prediction for Individual Models\n",
    "for name, model in models.items():\n",
    "    cat_features = []\n",
    "    X_train_mod, X_test_mod, cat_features = preprocess_for_model(name, X_train, X_test)\n",
    "\n",
    "    if name == \"CatBoost\":\n",
    "        model.fit(X_train_mod, y_train, cat_features=cat_features)\n",
    "\n",
    "    elif name == \"LightGBM\":\n",
    "        model.fit(X_train_mod, y_train, categorical_feature=cat_features)\n",
    "\n",
    "    elif name in [\"XGBoost\", \"RandomForest\", \"Ridge\", \"ElasticNet\", \"SVR\"]:\n",
    "        model.fit(X_train_mod, y_train)\n",
    "\n",
    "    preds = model.predict(X_test_mod)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name} R² score: {r2:.4f}\")\n",
    "\n",
    "    # importances = model.feature_importances_\n",
    "    # features = X_train_mod.columns\n",
    "    # sorted_indices = importances.argsort()[::-1]\n",
    "    \n",
    "    # Print top features\n",
    "    # for i in sorted_indices[:20]:\n",
    "    #     print(f\"{features[i]}: {importances[i]:.4f}\")\n",
    "\n",
    "    # print()\n",
    "\n",
    "\n",
    "\n",
    "# === Level 1 meta model Prediction===\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "meta_models = {\n",
    "    \"RidgeCV\": RidgeCV(\n",
    "        alphas=[0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        fit_intercept=True\n",
    "    )\n",
    "}\n",
    "X_train = pipeline.fit_transform(train_df)\n",
    "y_train = train_df['target']\n",
    "X_test = pipeline.fit_transform(test_df)\n",
    "\n",
    "\n",
    "# Out-of-fold predictions for train set\n",
    "train_meta = np.zeros((X_train.shape[0], len(models)))\n",
    "test_meta = np.zeros((X_test.shape[0], len(models)))\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    test_meta_fold = np.zeros((X_test.shape[0], n_folds))\n",
    "\n",
    "    # Preprocess X_test ONCE per model\n",
    "    X_test_preprocessed = preprocess_for_model(name, X_train, X_test)[1]\n",
    "\n",
    "\n",
    "    for j, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        X_tr_raw = X_train.iloc[train_idx]\n",
    "        X_val_raw = X_train.iloc[val_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "\n",
    "        # Preprocess fold data\n",
    "        cat_features = []\n",
    "        X_tr, X_val, cat_features = preprocess_for_model(name, X_tr_raw, X_val_raw)\n",
    "        # Align all 3 sets to ensure identical columns\n",
    "        X_tr, X_val = X_tr.align(X_val, join=\"outer\", axis=1, fill_value=0)\n",
    "        X_tr, X_test_preprocessed = X_tr.align(X_test_preprocessed, join=\"outer\", axis=1, fill_value=0)\n",
    "        X_val, X_test_preprocessed = X_val.align(X_test_preprocessed, join=\"outer\", axis=1, fill_value=0)\n",
    "\n",
    "        model_clone = clone(model)\n",
    "\n",
    "        if name == \"LightGBM\":\n",
    "            model_clone.fit(X_tr, y_tr, categorical_feature=cat_features)\n",
    "        elif name == \"CatBoost\":\n",
    "            model_clone.fit(X_tr, y_tr, cat_features=cat_features)\n",
    "        else:\n",
    "            model_clone.fit(X_tr, y_tr)\n",
    "\n",
    "        train_meta[val_idx, i] = model_clone.predict(X_val)\n",
    "        test_meta_fold[:, j] = model_clone.predict(X_test_preprocessed)\n",
    "\n",
    "    # Average test predictions across folds\n",
    "    test_meta[:, i] = test_meta_fold.mean(axis=1)\n",
    "\n",
    "# Fit and evaluate each meta-model\n",
    "for name, model in meta_models.items():\n",
    "    model.fit(train_meta, y_train)\n",
    "    stack_preds = model.predict(test_meta)\n",
    "\n",
    "    submit_df = pd.DataFrame({'id': range(len(stack_preds)), 'target':stack_preds})\n",
    "    submit_df.to_csv('submit.csv', index = False)\n",
    "    \n",
    "    # stack_r2 = r2_score(y_test, stack_preds)\n",
    "    # print(f\"Level 1 meta model {name} R² score: {stack_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784cd916-8580-431d-a75a-8cb7afcc6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After hyper parameter tuning\n",
    "CatBoost R² score: 0.8536\n",
    "XGBoost R² score: 0.8631\n",
    "LightGBM R² score: 0.8572\n",
    "RandomForest R² score: 0.7544\n",
    "Level 1 meta model RidgeCV R² score: 0.8716\n",
    "\n",
    "CatBoost R² score: 0.8110\n",
    "XGBoost R² score: 0.8114\n",
    "LightGBM R² score: 0.7715\n",
    "RandomForest R² score: 0.7542\n",
    "Level 1 meta model RidgeCV R² score: 0.8307\n",
    "\n",
    "CatBoost R² score: 0.8100\n",
    "XGBoost R² score: 0.8049\n",
    "LightGBM R² score: 0.7715\n",
    "RandomForest R² score: 0.7542\n",
    "Level 1 meta model RidgeCV R² score: 0.8332\n",
    "\n",
    "CatBoost R² score: 0.8047\n",
    "XGBoost R² score: 0.8049\n",
    "LightGBM R² score: 0.7715\n",
    "RandomForest R² score: 0.7542\n",
    "Level 1 meta model RidgeCV R² score: 0.8329\n",
    "\n",
    "CatBoost R² score: 0.8047\n",
    "XGBoost R² score: 0.7980\n",
    "LightGBM R² score: 0.7715\n",
    "RandomForest R² score: 0.7542\n",
    "Level 1 meta model RidgeCV R² score: 0.8353\n",
    "\n",
    "CatBoost R² score: 0.8019\n",
    "XGBoost R² score: 0.8049\n",
    "LightGBM R² score: 0.7673\n",
    "RandomForest R² score: 0.7560\n",
    "Level 1 meta model RidgeCV R² score: 0.8305\n",
    "\n",
    "CatBoost R² score: 0.8019\n",
    "XGBoost R² score: 0.7932\n",
    "LightGBM R² score: 0.7673\n",
    "RandomForest R² score: 0.7560\n",
    "Level 1 meta model RidgeCV R² score: 0.8314\n",
    "\n",
    "CatBoost R² score: 0.8019\n",
    "XGBoost R² score: 0.7928\n",
    "LightGBM R² score: 0.7673\n",
    "RandomForest R² score: 0.7560\n",
    "Level 1 meta model RidgeCV R² score: 0.8315\n",
    "\n",
    "CatBoost R² score: 0.7993\n",
    "XGBoost R² score: 0.7906\n",
    "LightGBM R² score: 0.7524\n",
    "RandomForest R² score: 0.7547\n",
    "Level 1 meta model RidgeCV R² score: 0.8275\n",
    "\n",
    "# After station info data exclusion\n",
    "CatBoost R² score: 0.7982\n",
    "XGBoost R² score: 0.7880\n",
    "LightGBM R² score: 0.7557\n",
    "RandomForest R² score: 0.7557\n",
    "Level 1 meta model RidgeCV R² score: 0.8277\n",
    "\n",
    "# With day of year\n",
    "CatBoost R² score: 0.8095\n",
    "XGBoost R² score: 0.7996\n",
    "LightGBM R² score: 0.7635\n",
    "RandomForest R² score: 0.7333\n",
    "Ridge R² score: 0.6655\n",
    "ElasticNet R² score: 0.6367\n",
    "Level 1 meta model RidgeCV R² score: 0.8330\n",
    "\n",
    "# with no day of year and sin cos\n",
    "CatBoost R² score: 0.8054\n",
    "XGBoost R² score: 0.7965\n",
    "LightGBM R² score: 0.7573\n",
    "RandomForest R² score: 0.7298\n",
    "Ridge R² score: 0.6655\n",
    "ElasticNet R² score: 0.6367\n",
    "Level 1 meta model RidgeCV R² score: 0.8331\n",
    "\n",
    "CatBoost R² score: 0.8082\n",
    "XGBoost R² score: 0.7996\n",
    "LightGBM R² score: 0.7635\n",
    "RandomForest R² score: 0.7348\n",
    "Ridge R² score: 0.6655\n",
    "ElasticNet R² score: 0.6367\n",
    "Level 1 meta model RidgeCV R² score: 0.833\n",
    "\n",
    "# After station info addition\n",
    "CatBoost R² score: 0.8066\n",
    "XGBoost R² score: 0.8005\n",
    "LightGBM R² score: 0.7640\n",
    "RandomForest R² score: 0.7348\n",
    "Ridge R² score: 0.6626\n",
    "ElasticNet R² score: 0.6351\n",
    "Level 1 meta model RidgeCV R² score: 0.8336\n",
    "\n",
    "# Cat boost depth and iteration addition\n",
    "CatBoost R2 score: 0.8433\n",
    "XGBoost R2 score: 0.8005\n",
    "LightGBM R2 score: 0.7640\n",
    "RandomForest R2 score: 0.7348\n",
    "Ridge R2 score: 0.6626\n",
    "ElasticNet R2 score: 0.6351\n",
    "Level 1 meta model RidgeCV R2 score: 0.8540\n",
    "Level 2 input model RidgeCV R2 score: 0.8330\n",
    "\n",
    "\n",
    "CatBoost R² score: 0.8016\n",
    "XGBoost R² score: 0.7973\n",
    "LightGBM R² score: 0.7582\n",
    "RandomForest R² score: 0.7570\n",
    "Ridge R² score: 0.6595\n",
    "ElasticNet R² score: 0.6352\n",
    "Level 1 meta model RidgeCV R² score: 0.8284\n",
    "\n",
    "# Without day of year\n",
    "CatBoost R² score: 0.7992\n",
    "XGBoost R² score: 0.7830\n",
    "LightGBM R² score: 0.7503\n",
    "RandomForest R² score: 0.7512\n",
    "Ridge R² score: 0.6595\n",
    "ElasticNet R² score: 0.6352\n",
    "Level 1 meta model RidgeCV R² score: 0.8264\n",
    "\n",
    "# Without vapor pressure\n",
    "CatBoost R² score: 0.7999\n",
    "XGBoost R² score: 0.7864\n",
    "LightGBM R² score: 0.7544\n",
    "RandomForest R² score: 0.7584\n",
    "Ridge R² score: 0.6587\n",
    "ElasticNet R² score: 0.6315\n",
    "Level 1 meta model RidgeCV R² score: 0.8292\n",
    "\n",
    "CatBoost R² score: 0.8016\n",
    "XGBoost R² score: 0.7973\n",
    "LightGBM R² score: 0.7582\n",
    "RandomForest R² score: 0.7570\n",
    "Level 1 meta model RidgeCV R² score: 0.8278\n",
    "\n",
    "CatBoost R² score: 0.8016\n",
    "XGBoost R² score: 0.7909\n",
    "LightGBM R² score: 0.7554\n",
    "RandomForest R² score: 0.7570\n",
    "Level 1 meta model RidgeCV R² score: 0.8281\n",
    "\n",
    "CatBoost R² score: 0.7985\n",
    "XGBoost R² score: 0.7909\n",
    "LightGBM R² score: 0.7554\n",
    "RandomForest R² score: 0.7570\n",
    "Level 1 meta model RidgeCV R² score: 0.8273\n",
    "\n",
    "CatBoost R² score: 0.7967\n",
    "XGBoost R² score: 0.7571\n",
    "LightGBM R² score: 0.7450\n",
    "RandomForest R² score: 0.7538\n",
    "Level 1 meta model RidgeCV R² score: 0.8117\n",
    "\n",
    "CatBoost R² score: 0.7855\n",
    "XGBoost R² score: 0.7973\n",
    "LightGBM R² score: 0.7582\n",
    "RandomForest R² score: 0.7482\n",
    "Manual Stacking RidgeCV R² score: 0.8226\n",
    "\n",
    "CatBoost R² score: 0.7861\n",
    "XGBoost R² score: 0.7944\n",
    "LightGBM R² score: 0.7533\n",
    "RandomForest R² score: 0.7432\n",
    "Manual Stacking RidgeCV R² score: 0.8234\n",
    "\n",
    "CatBoost R² score: 0.7844\n",
    "XGBoost R² score: 0.7944\n",
    "LightGBM R² score: 0.7533\n",
    "RandomForest R² score: 0.7424\n",
    "Manual Stacking RidgeCV R² score: 0.8220\n",
    "\n",
    "CatBoost R² score: 0.7844\n",
    "XGBoost R² score: 0.7944\n",
    "LightGBM R² score: 0.7533\n",
    "RandomForest R² score: 0.7408\n",
    "Manual Stacking RidgeCV R² score: 0.8220\n",
    "\n",
    "\n",
    "CatBoost R² score: 0.7844\n",
    "XGBoost R² score: 0.7944\n",
    "LightGBM R² score: 0.7533\n",
    "Manual Stacking RidgeCV R² score: 0.8218\n",
    "\n",
    "CatBoost R² score: 0.7797\n",
    "XGBoost R² score: 0.7944\n",
    "LightGBM R² score: 0.7533\n",
    "Manual Stacking RidgeCV R² score: 0.8185\n",
    "\n",
    "CatBoost R² score: 0.7709\n",
    "XGBoost R² score: 0.7847\n",
    "LightGBM R² score: 0.7505\n",
    "Manual Stacking RidgeCV R² score: 0.8097\n",
    "\n",
    "CatBoost R² score: 0.7693\n",
    "XGBoost R² score: 0.7911\n",
    "LightGBM R² score: 0.7434\n",
    "Manual Stacking RidgeCV R² score: 0.8100\n",
    "\n",
    "CatBoost R² score: 0.7693\n",
    "XGBoost R² score: 0.7911\n",
    "LightGBM R² score: 0.7434\n",
    "Stacked RidgeCV R² score: 0.8113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b7b05ef-e1d7-451b-9db5-c5974551dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range Histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histograms_grid(df, bins=50, cols_per_row=5, figsize=(20, 4)):\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    num_cols = len(numeric_cols)\n",
    "    num_rows = int(np.ceil(num_cols / cols_per_row))\n",
    "    \n",
    "    plt.figure(figsize=(figsize[0], figsize[1] * num_rows))\n",
    "\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        plt.subplot(num_rows, cols_per_row, i + 1)\n",
    "        plt.hist(df[col].dropna(), bins=bins, edgecolor='black')\n",
    "        plt.title(col)\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "X_train_ori = pipeline.fit_transform(train_df)\n",
    "# plot_histograms_grid(X_train_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d483e3e-91b5-4d1f-8c6f-8dd6963eab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graveyard\n",
    "\n",
    "# Level 2 training meta-data\n",
    "n_splits = 5\n",
    "level2_train_meta = np.zeros((train_meta.shape[0], len(meta_models)))\n",
    "level2_test_meta = np.zeros((X_test.shape[0], len(meta_models)))\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (name, model) in enumerate(meta_models.items()):\n",
    "    level2_test_meta_fold = np.zeros((X_test.shape[0], n_splits))\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(train_meta)):\n",
    "        X_tr_meta, X_val_meta = train_meta[tr_idx], train_meta[val_idx]\n",
    "        y_tr_meta, y_val_meta = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model_clone = clone(model)\n",
    "        model_clone.fit(X_tr_meta, y_tr_meta)\n",
    "\n",
    "        # Predict for validation fold\n",
    "        level2_train_meta[val_idx, i] = model_clone.predict(X_val_meta)\n",
    "\n",
    "        # Predict for test set\n",
    "        level2_test_meta_fold[:, fold] = model_clone.predict(test_meta)\n",
    "\n",
    "    # Average predictions across folds for final test_meta\n",
    "    level2_test_meta[:, i] = level2_test_meta_fold.mean(axis=1)\n",
    "\n",
    "for name, model in meta_models.items():\n",
    "    # Clone to avoid contamination from previous folds\n",
    "    model_clone = clone(model)\n",
    "    \n",
    "    # Fit on full level 2 training set\n",
    "    model_clone.fit(level2_train_meta, y_train)\n",
    "    \n",
    "    # Predict on level 2 test meta\n",
    "    preds = model_clone.predict(level2_test_meta)\n",
    "    \n",
    "    # Calculate R²\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    \n",
    "    print(f\"Level 2 meta model {name:10} R²: {r2:.4f}\")\n",
    "\n",
    "    # r2 = r2_score(y_train, level2_train_meta[:, i])\n",
    "    # print(f\"Level 2 meta model {name} R² score: {r2:.4f}\")\n",
    "\n",
    "# # === STACKED MODEL StackingRegressor===\n",
    "# # Wrap individual models for stacking\n",
    "# estimators = [\n",
    "#     ('cat', models[\"CatBoost\"]),\n",
    "#     ('xgb', models[\"XGBoost\"]),\n",
    "#     ('lgb', models[\"LightGBM\"])\n",
    "# ]\n",
    "\n",
    "# meta_model = RidgeCV()\n",
    "\n",
    "# stack = StackingRegressor(\n",
    "#     estimators=estimators,\n",
    "#     final_estimator=meta_model,\n",
    "#     cv=5  # OOF stacking\n",
    "# )\n",
    "\n",
    "# # Train stacking model\n",
    "# stack.fit(X_train, y_train)\n",
    "# stack_preds = stack.predict(X_test)\n",
    "# stack_r2 = r2_score(y_test, stack_preds)\n",
    "# print(f\"StackingRegressor RidgeCV R² score: {stack_r2:.4f}\")\n",
    "\n",
    "\n",
    "# # === Level 2 meta model prediction ===\n",
    "\n",
    "# n_folds = 5\n",
    "# kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# # Prepare placeholders\n",
    "# level2_train = np.zeros((train_meta.shape[0], len(meta_models)))\n",
    "# level2_test = np.zeros((test_meta.shape[0], len(meta_models), n_folds))\n",
    "\n",
    "# # For each fold, fill in level2_train and test predictions\n",
    "# for fold, (tr_idx, val_idx) in enumerate(kf.split(train_meta)):\n",
    "#     print(f\"LEVEL 2 FOLD {fold+1}/{n_folds}\")\n",
    "    \n",
    "#     X_tr, X_val = train_meta[tr_idx], train_meta[val_idx]\n",
    "#     y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n",
    "\n",
    "#     for i, (name, model) in enumerate(meta_models.items()):\n",
    "#         model_clone = clone(model)\n",
    "#         model_clone.fit(X_tr, y_tr)\n",
    "\n",
    "#         level2_train[val_idx, i] = model_clone.predict(X_val)\n",
    "#         level2_test[:, i, fold] = model_clone.predict(test_meta)\n",
    "\n",
    "# # Average test predictions across folds\n",
    "# level2_test_mean = np.mean(level2_test, axis=2)\n",
    "\n",
    "# # Now train final level 2 model on OOF predictions\n",
    "# final_model = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0])\n",
    "# final_model.fit(level2_train, y_train)\n",
    "\n",
    "# final_oof_preds = final_model.predict(level2_train)\n",
    "# final_test_preds = final_model.predict(level2_test_mean)\n",
    "\n",
    "# # Evaluation\n",
    "# final_r2 = r2_score(y_train, final_oof_preds)\n",
    "# print(f\"Level 2 Meta Model RidgeCV R²: {final_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "29fec31e-eb3a-4d30-8751-8e4edfdef479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code from now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39852ed9-b2b6-4dfe-a155-3d1af69cf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Code\n",
    "\n",
    "X_train_ori = pipeline.fit_transform(train_df)\n",
    "y_test_ori = train_df['target']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_ori, y_test_ori, test_size = 0.2, random_state=42)\n",
    "\n",
    "# STEP 4: Define models\n",
    "models = {\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=100, random_state=42, verbose = -1)\n",
    "}\n",
    "\n",
    "# STEP 5: Train and evaluate individual models\n",
    "for name, model in models.items():\n",
    "    if name == \"CatBoost\":\n",
    "        model.fit(X_train, y_train, cat_features=cat_features)\n",
    "    elif name == \"LightGBM\":\n",
    "        model.fit(X_train, y_train, categorical_feature=cat_features)\n",
    "    else:  # XGBoost\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name} R² score: {r2:.4f}\")\n",
    "\n",
    "# === STACKED MODEL ===\n",
    "# Wrap individual models for stacking\n",
    "estimators = [\n",
    "    ('cat', models[\"CatBoost\"]),\n",
    "    ('xgb', models[\"XGBoost\"]),\n",
    "    ('lgb', models[\"LightGBM\"])\n",
    "]\n",
    "\n",
    "meta_model = RidgeCV()\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5  # OOF stacking\n",
    ")\n",
    "\n",
    "# Train stacking model\n",
    "stack.fit(X_train, y_train)\n",
    "stack_preds = stack.predict(X_test)\n",
    "stack_r2 = r2_score(y_test, stack_preds)\n",
    "print(f\"Stacked RidgeCV R² score: {stack_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595d0e3-6302-4fee-91fd-8d8be62344ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ori = pipeline.fit_transform(train_df)\n",
    "y_test_ori = train_df['target']\n",
    "kaka = pipeline.fit_transform(test_df)\n",
    "\n",
    "stack.fit(X_train_ori, y_test_ori)\n",
    "submit_preds = stack.predict(kaka)\n",
    "submit_df = pd.DataFrame({\n",
    "    'id': range(len(submit_preds)),\n",
    "    'target': submit_preds\n",
    "})\n",
    "submit_df.head(200)\n",
    "submit_df.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfab13-9e80-447d-8438-ef5608db8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Condensed Features + Original Features (About 400 features)\n",
    "\n",
    "X_train_ori = pipeline.fit_transform(train_df)\n",
    "y_test_ori = train_df['target']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_ori, y_test_ori, test_size = 0.2, random_state=42)\n",
    "\n",
    "# STEP 4: Define models\n",
    "models = {\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=100, random_state=42, verbose = -1)\n",
    "}\n",
    "\n",
    "# STEP 5: Train and evaluate individual models\n",
    "for name, model in models.items():\n",
    "    if name == \"CatBoost\":\n",
    "        model.fit(X_train, y_train, cat_features=cat_features)\n",
    "    elif name == \"LightGBM\":\n",
    "        model.fit(X_train, y_train, categorical_feature=cat_features)\n",
    "    else:  # XGBoost\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name} R² score: {r2:.4f}\")\n",
    "\n",
    "# === STACKED MODEL ===\n",
    "# Wrap individual models for stacking\n",
    "estimators = [\n",
    "    ('cat', models[\"CatBoost\"]),\n",
    "    ('xgb', models[\"XGBoost\"]),\n",
    "    ('lgb', models[\"LightGBM\"])\n",
    "]\n",
    "\n",
    "meta_model = RidgeCV()\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5  # OOF stacking\n",
    ")\n",
    "\n",
    "# Train stacking model\n",
    "stack.fit(X_train, y_train)\n",
    "stack_preds = stack.predict(X_test)\n",
    "stack_r2 = r2_score(y_test, stack_preds)\n",
    "print(f\"Stacked RidgeCV R² score: {stack_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fda765c-9cb6-49ef-a80b-ead9d7ed2cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost R² score: 0.7739\n",
      "XGBoost R² score: 0.7319\n",
      "LightGBM R² score: 0.7280\n"
     ]
    }
   ],
   "source": [
    "# No feature engineering\n",
    "\n",
    "# STEP 0: Define dropped columns\n",
    "drop_cols = [\"target\", \"id\", \"station\", \"station_name\"]\n",
    "\n",
    "# STEP 1: Extract month & day from date column (assumed to be named 'date' in MM-DD format)\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df[\"month\"] = df[\"date\"].str.split(\"-\").str[0].astype(int)\n",
    "    df[\"day\"] = df[\"date\"].str.split(\"-\").str[1].astype(int)\n",
    "    df = df.drop(columns=[\"date\"] + [col for col in drop_cols if col in df.columns])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# STEP 2: Prepare data\n",
    "X_train_ori = preprocess(train_df)\n",
    "# X_test = preprocess(test_df)\n",
    "y_train_ori = train_df[\"target\"]\n",
    "# y_test = test_df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_ori, y_train_ori, test_size = 0.2, random_state=42)\n",
    "\n",
    "\n",
    "# STEP 3: Detect categorical columns\n",
    "cat_features = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# STEP 4: Define models\n",
    "models = {\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=100, random_state=42, verbose = -1),\n",
    "}\n",
    "\n",
    "# STEP 5: Train and evaluate\n",
    "for name, model in models.items():\n",
    "    if name == \"CatBoost\":\n",
    "        model.fit(X_train, y_train, cat_features=cat_features)\n",
    "    elif name == \"LightGBM\":\n",
    "        model.fit(X_train, y_train, categorical_feature=cat_features)\n",
    "    else:  # XGBoost\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name} R² score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26cc43-fa63-4edb-a51c-85968d066192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f017d15a-91b8-465a-9e6d-8119fce53b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STEP 0: Define dropped columns\n",
    "drop_cols = [\"target\", \"id\", \"station\", \"station_name\"]\n",
    "\n",
    "# STEP 1: Extract month & day from date column (assumed to be named 'date' in MM-DD format)\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df[\"month\"] = df[\"date\"].str.split(\"-\").str[0].astype(int)\n",
    "    df[\"day\"] = df[\"date\"].str.split(\"-\").str[1].astype(int)\n",
    "    df = df.drop(columns=[\"date\"] + [col for col in drop_cols if col in df.columns])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# STEP 2: Prepare data\n",
    "X_train = preprocess(train_df)\n",
    "# X_test = preprocess(test_df)\n",
    "y_train = train_df[\"target\"]\n",
    "# y_test = test_df[\"target\"]\n",
    "x_pre = preprocess(test_df)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train_ori, y_train_ori, test_size = 0.2, random_state=42)\n",
    "\n",
    "\n",
    "# STEP 3: Detect categorical columns\n",
    "cat_features = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = CatBoostRegressor(verbose=0, random_state=42)\n",
    "model.fit(X_train, y_train, cat_features=cat_features)\n",
    "pred = model.predict(x_pre)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": range(len(pred)),     # generate sequential IDs: 0, 1, 2, ...\n",
    "    \"target\": pred              # predicted values\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv(\"cat_boost.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# STEP 4: Define models\n",
    "# models = {\n",
    "#     \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42),\n",
    "#     # \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42),\n",
    "#     # \"LightGBM\": LGBMRegressor(n_estimators=100, random_state=42),\n",
    "# }\n",
    "\n",
    "# STEP 5: Train and evaluate\n",
    "# for name, model in models.items():\n",
    "#     if name == \"CatBoost\":\n",
    "#         model.fit(X_train, y_train, cat_features=cat_features)\n",
    "#     # elif name == \"LightGBM\":\n",
    "#     #     model.fit(X_train, y_train, categorical_feature=cat_features)\n",
    "#     # else:  # XGBoost\n",
    "#     #     model.fit(X_train, y_train)\n",
    "\n",
    "#     preds = model.predict(X_test)\n",
    "#     r2 = r2_score(y_test, preds)\n",
    "#     print(f\"{name} R² score: {r2:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02e8e965-0184-479e-8940-8de7391ee418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        01-01\n",
       "1        01-02\n",
       "2        01-03\n",
       "3        01-04\n",
       "4        01-05\n",
       "         ...  \n",
       "13127    12-26\n",
       "13128    12-27\n",
       "13129    12-28\n",
       "13130    12-29\n",
       "13131    12-30\n",
       "Name: date, Length: 13132, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target, id, station, station_name\n",
    "train_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931a4b0-a8a2-4c45-9248-69881be939fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167a182-1c43-46cb-9d8a-4a2fdc1e10db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5e278-c48c-443c-9d6e-fcd8094e00e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f2594-356d-4918-a340-72789df8d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor  # ← skip this line if not installed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume: train_df, test_df, y_train, y_test are ready and numeric\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=100, random_state=42),  # ← skip if not installed\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(train_df, y_train)\n",
    "    preds = model.predict(test_df)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "    print(f\"{name} RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80fb65bd-59f4-431b-8063-9a901e8e4b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca86aec-2ab3-4ec8-b3bb-05dbe311d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(pipeline, train_df, train_y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"CV RMSE:\", np.mean(np.sqrt(-scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52e4fd-4328-44f0-99e4-58284702542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_cover_cols = [f\"cloud_cover_{i}\" for i in range(24)]\n",
    "humidity_cols = [f\"humidity_{i}\" for i in range(24)]\n",
    "\n",
    "feature_definitions ={\n",
    "\"cloud_cover_n\": cloud_cover_cols,\n",
    "\"humidity_n\": humidity_cols,\n",
    "}\n",
    "feature_config = {\n",
    "\"cloud_cover_n\": {\"total_sum\": True, \"skew\": True},\n",
    "\"humidity_n\": {\"total_sum\": False, \"skew\": False},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030e56e-949f-411e-87eb-918feb2a38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Condenser:\n",
    "    def __init__(self, feature_name, hour_cols, config):\n",
    "        self.feature = feature_name\n",
    "        self.hour_cols = hour_cols\n",
    "        self.config = config  # Dict of which features to generate (from the table)\n",
    "\n",
    "    def transform(self, df):\n",
    "        values = df[self.hour_cols].copy()\n",
    "        result = pd.DataFrame(index=df.index)\n",
    "\n",
    "        if self.config.get(\"total_sum\"):\n",
    "            result[f\"{self.feature}_total_sum\"] = values.sum(axis=1, skipna=True)\n",
    "\n",
    "        if self.config.get(\"has_condition\"):\n",
    "            result[f\"{self.feature}_has_condition\"] = (values > 0).any(axis=1).astype(int)\n",
    "\n",
    "        if self.config.get(\"valid_count\"):\n",
    "            result[f\"{self.feature}_valid_count\"] = values.count(axis=1)\n",
    "\n",
    "        # Continue for other flags like:\n",
    "        if self.config.get(\"segment_stats\"):\n",
    "            result = result.join(self._get_segment_stats(values))\n",
    "\n",
    "        if self.config.get(\"global_stats\"):\n",
    "            result = result.join(self._get_global_stats(values))\n",
    "\n",
    "        if self.config.get(\"skewness\"):\n",
    "            result[f\"{self.feature}_skewness\"] = values.apply(\n",
    "                lambda row: row.skew() if row.count() >= 5 else np.nan, axis=1\n",
    "            )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _get_segment_stats(self, values):\n",
    "    # Morning, Afternoon, Evening, Night segment breakdown\n",
    "    # Return DataFrame with segment-based mean, std, range, etc.\n",
    "    ...\n",
    "\n",
    "    def _get_global_stats(self, values):\n",
    "    # Calculate mean, std, range, hour_max_jump etc.\n",
    "    # Return as DataFrame\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286bd94-65c4-4fca-ad79-13fbdca4c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "condensed_dfs = []\n",
    "\n",
    "for feature, hour_cols in feature_definitions.items():\n",
    "    config = feature_config[feature]\n",
    "    condenser = Condenser(feature, hour_cols, config)\n",
    "    condensed_df = condenser.transform(train_df)\n",
    "    condensed_dfs.append(condensed_df)\n",
    "    \n",
    "condesnsed_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcc087cb-86e4-43fe-b608-9cfa32cf8b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=13132, step=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6m7NGngYUue0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6m7NGngYUue0",
    "outputId": "d0be1360-5733-4911-f283-9fb07a297063"
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# train_df.head(500).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U0RrJ4_MaGJU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0RrJ4_MaGJU",
    "outputId": "a279876d-b5e1-4072-c0da-a64a5e4947c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XBBOpXxskOYa",
   "metadata": {
    "id": "XBBOpXxskOYa"
   },
   "outputs": [],
   "source": [
    "# Features\n",
    "\n",
    "## Original featuers\n",
    "# id: 순서\n",
    "# station: 지상관측소 번호\n",
    "# station_name: 지상관측소 이름\n",
    "# date: 날짜(월-일)\n",
    "# cloud_cover_n: 증하층운량(10분위)\n",
    "# dew_point_n: 이슬점 온도(°C)\n",
    "# humidity_n: 습도(%)\n",
    "# local_pressure_n: 현지기압(hPa)m\n",
    "# min_cloud_height_n: 최저운고(100m)\n",
    "# precipitation_n: 강수량(mm)\n",
    "# sea_level_pressure_n: 해면기압(hPa)\n",
    "# snow_depth_n: 적설(cm)\n",
    "# sunshine_duration_n: 일조(hr)\n",
    "# surface_temp_n: 지면온도(°C)\n",
    "# vapor_pressure_n: 증기압(hPa)\n",
    "# visibility_n: 시정(10m)\n",
    "# wind_speed_n: 풍속(m/s)\n",
    "# wind_direction_n: 풍향(°)\n",
    "# climatology_temp:\n",
    "\n",
    "## Added features\n",
    "# year\n",
    "\n",
    "\n",
    "## Utilizing features\n",
    "# cloud_cover_n: 증하층운량(10분위)\n",
    "# dew_point_n: 이슬점 온도(°C)\n",
    "# humidity_n: 습도(%)\n",
    "# local_pressure_n: 현지기압(hPa)\n",
    "# min_cloud_height_n: 최저운고(100m)\n",
    "# precipitation_n: 강수량(mm)\n",
    "# sea_level_pressure_n: 해면기압(hPa)\n",
    "# snow_depth_n: 적설(cm)\n",
    "# sunshine_duration_n: 일조(hr)\n",
    "# surface_temp_n: 지면온도(°C)\n",
    "# vapor_pressure_n: 증기압(hPa)\n",
    "# visibility_n: 시정(10m)\n",
    "# wind_speed_n: 풍속(m/s)\n",
    "# wind_direction_n: 풍향(°)\n",
    "# climatology_temp:\n",
    "# year\n",
    "# is_weekend\n",
    "\n",
    "# is_weekend, day, month, wind_direction convert\n",
    "# features with nan convert\n",
    "# station longitude, latitude\n",
    "\n",
    "\n",
    "## Omit features\n",
    "# id, station, station_name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fePPAGqhMTD1",
   "metadata": {
    "id": "fePPAGqhMTD1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def is_leap_year(year):\n",
    "  return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n",
    "\n",
    "def add_year_column(df, year = 2019):\n",
    "  year_added_df = df.copy()\n",
    "  years_column = []\n",
    "  current_year = year -1\n",
    "  for station_id, station_df in year_added_df.groupby('station'):\n",
    "    for mmdd in station_df['date']:\n",
    "      if mmdd == '01-01':\n",
    "        current_year += 1\n",
    "      years_column.append(current_year)\n",
    "    current_year = year-1\n",
    "  year_added_df['year'] = years_column\n",
    "  return year_added_df\n",
    "\n",
    "def list_n_24_add(name):\n",
    "  n_24 = []\n",
    "  for x in range(24):\n",
    "    n_24.append(name +f'_{x}')\n",
    "  return n_24\n",
    "\n",
    "# Add year column\n",
    "year_train_df = add_year_column(train_df)\n",
    "test_df = add_year_column(test_df)\n",
    "\n",
    "# Wind_direction\n",
    "wind_directions = list_n_24_add('wind_direction')\n",
    "wind_speeds = list_n_24_add('wind_speed')\n",
    "\n",
    "# columns rearrange\n",
    "col_all = list(year_train_df.columns)\n",
    "col_im = ['id', 'date', 'year', 'station', 'station_name', 'climatology_temp', 'target']\n",
    "col_other = natsorted([c for c in col_all if c not in col_im])\n",
    "edited_col = col_im + col_other\n",
    "\n",
    "# df column rearrange\n",
    "year_train_df = year_train_df[edited_col].copy()\n",
    "year_target_df = year_train_df['target'].copy()\n",
    "year_train_df.drop(['id', 'date', 'station', 'station_name', 'target'] + wind_directions + wind_speeds, axis = 1, inplace=True)\n",
    "\n",
    "test_df = test_df[['id', 'date', 'year', 'station', 'station_name', 'climatology_temp']+col_other].copy()\n",
    "test_df.drop(['id', 'date', 'station', 'station_name'] + wind_directions + wind_speeds, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bxgfdEHSysW_",
   "metadata": {
    "id": "bxgfdEHSysW_"
   },
   "outputs": [],
   "source": [
    "# Convert nan to separate column and set the value to -1\n",
    "def nan_handle(df):\n",
    "  updates = {}\n",
    "\n",
    "  for x in df.columns:\n",
    "    if df[x].isna().sum() > 0:\n",
    "      updates[f'{x}_is_nan'] = df[x].isna().astype(int)\n",
    "      df[x] = df[x].fillna(-1)\n",
    "\n",
    "  df = pd.concat([df, pd.DataFrame(updates)], axis = 1)\n",
    "  return df\n",
    "\n",
    "year_train_df = nan_handle(year_train_df)\n",
    "test_df = nan_handle(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owhLekkm82uh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owhLekkm82uh",
    "outputId": "7090885f-dea6-4042-eb0a-5c26d388eeba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check if there's any nan value\n",
    "print((year_train_df.isna().sum() > 0).any())\n",
    "print((test_df.isna().sum() > 0).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EXSUEqrxywaS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXSUEqrxywaS",
    "outputId": "f58a83bd-8d05-4228-9018-f224b9c4725b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse is 2.471875641788321\n",
      "r2 score is 0.7160905189217839\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(year_train_df, year_target_df, test_size = 0.3, random_state=67)\n",
    "graaa = RandomForestRegressor(n_estimators = 100, max_depth = None, random_state = 67, n_jobs = -1)\n",
    "graaa.fit(x_train, y_train)\n",
    "\n",
    "y_pred = graaa.predict(x_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'mse is {mse}')\n",
    "print(f'r2 score is {r2}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
